{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a819d766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pygraphviz\n",
      "Version: 1.14\n",
      "Summary: Python interface to Graphviz\n",
      "Home-page: https://pygraphviz.github.io\n",
      "Author: Manos Renieris\n",
      "Author-email: Aric Hagberg <aric.hagberg@gmail.com>, Dan Schult <dschult@colgate.edu>\n",
      "License: BSD-3-Clause\n",
      "Location: /home/jain/anaconda3/envs/myenv/lib/python3.12/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# sudo apt-get update\n",
    "# sudo apt-get install graphviz graphviz-dev\n",
    "\n",
    "! pip show pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e199ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "848bdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/jain/Desktop/ws/api_keys_in_laptop.json\", \"r\") as f:\n",
    "    api_keys = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1c54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bab4f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    tavily_responses: List[str]\n",
    "    referred_urls: List[str]\n",
    "    initial_draft: str\n",
    "    feedback: str\n",
    "    final_draft: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf48fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "tavily = TavilyClient(api_key=api_keys[\"tavily\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6d27bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_tavily(state):\n",
    "    \"\"\"Query Tavily for relevant articles.\"\"\"\n",
    "    response = tavily.search(\n",
    "        query=state['query'], \n",
    "        include_answer=\"advanced\",\n",
    "        include_raw_content=\"text\",\n",
    "        max_results=10\n",
    "    )\n",
    "    content = []\n",
    "    urls = []\n",
    "    for r in response['results']:\n",
    "        # print(r)\n",
    "        if r['raw_content'] is not None:\n",
    "            content.append(r['raw_content'])\n",
    "            urls.append(r['url']\n",
    "            )\n",
    "    return {\"query\": state['query'], \"tavily_responses\": content, \"referred_urls\": urls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test query_tavily()\n",
    "\n",
    "# state = {\"query\": \"latest updates on development going on Gurgaon/Gurugram Railway Station\"}\n",
    "# state = query_tavily(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17de8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'tavily_responses': ['Quantum Insider Intelligence\\nAccelerate your quantum strategy\\nGain a 360-degree view of the quantum landscape\\u2028 to fuel your strategy\\n\\nTrusted by:\\n\\nQuantum Computing News\\n\\nXanadu Expands Partnership with ASTAR to Advance Photonic Quantum Computing\\n\\nAutocrypt Announces Release of Post-Quantum PKI Product for Automotive OEMs\\n\\nIonQ Expands in EU With Slovakia’s First National Quantum Communication Network\\n\\nQuantum Source Report Outlines Engineering Pathways to Fault-Tolerant Quantum Computing\\n\\nNew Zealand Partners With Korea on Quantum Communication Projects\\nFeatured Articles\\n\\nQuantum Source Report Outlines Engineering Pathways to Fault-Tolerant Quantum Computing\\n\\nQuantum Computing Companies in 2025: Mapping the Global Quantum Landscape\\n\\nDeterministic Entanglement for Photonic Quantum Computing: Atom as Interface\\n\\nWQD 25: 12 Events For World Quantum Day 2025\\n\\nQuantum Source’s Scalable Photon-Atom Technology Enables Practical Quantum Computing\\n\\nKeep track of the Quantum Technology Market.\\nIn one place.\\nQuantum Business News\\u200b\\n\\nAutocrypt Announces Release of Post-Quantum PKI Product for Automotive OEMs\\n\\nIonQ Expands in EU With Slovakia’s First National Quantum Communication Network\\n\\nXanadu Expands Partnership with ASTAR to Advance Photonic Quantum Computing\\nAI Insider PRESS RELEASE — Xanadu Quantum Technologies Inc. (“Xanadu”), a leading photonic quantum computing company, today announced it has signed a Memorandum of Understanding (“MoU”) with the\\n\\nQuantum Source Report Outlines Engineering Pathways to Fault-Tolerant Quantum Computing\\n\\nWhat Is The Price Of A Quantum Computer In 2025?\\nQuantum Computing Exclusives\\n\\nQuantum World Tour: Brazil Demonstrates National Vision Fueling An Emerging Quantum Ecosystem\\nInsider Brief: The Quantum World Tour, a global initiative launched by the International Telecommunication Union (ITU) in partnership with The Quantum Insider, will host its\\nQuantum World Tour Saudi Arabia: Inside the Kingdom’s Quantum Expansion\\nQuantum World Tour: Brazil Demonstrates National Vision Fueling An Emerging Quantum Ecosystem\\nTelecom at the Edge of Scale: How Quantum Technologies Are Recasting the Network Economy\\nDefence Panel Discusses How to Bring Quantum & AI to Bear in a Hostile World\\nQuantum World Tour Heads to Australia, Showcasing Strategy, Science, and Emerging Industry Strengths\\n\\nQuantum World Tour Debuts in Malta, Europe’s Emerging Quantum Hub\\n\\nThe Quantum Economy Podcast: Audacity in EU Quantum with Théau Peronnin and Anders Indset\\n\\nQuantum Marketing Solutions\\nDigital marketing campaigns for the world’s leading Quantum Technology companies.\\n\\nQuantum Industry Intelligence\\nThe leading provider of information, data, and insights on Quantum Technologies.\\nQuantum Interviews\\n\\nTQI Exclusive: Microsoft’s Krysta Svore Says Geometric Error-Correcting Codes Are a Step Toward Practical Applications\\n\\nWhat’s Quantum Biology? A Research Pioneer Shares His Vision for Quantum Technology’s Next Frontier\\n\\nStartup Fortaegis Says Its Hardware is Ready For Today’s Security Challenges – Perfectly Positioned For Tomorrow’s Quantum Era\\n\\nTQI Exclusive: Main Sequence’s Latest Thesis Guides Quantum Bets on the Edge of Commercial Use\\n\\nQuantum Source’s Scalable Photon-Atom Technology Enables Practical Quantum Computing\\n\\nSony Ventures Looks to Quantum and Renewable Energy for Scalable Deep Tech Investments\\nCapital Markets News\\n\\nXanadu Expands Partnership with ASTAR to Advance Photonic Quantum Computing\\n\\nHorizon Quantum Secures $110 Million PIPE, With IonQ Among Lead Investors, to Support SPAC Merger\\n\\nSEALSQ Makes Strategic Investment in EeroQ\\n\\nParityQC Awarded Contract by DLR to Integrate Quantum Computing for Next-Generation Mobility Solutions\\n\\nNTT Focuses on Light For Cleaner, Scalable Path to Quantum Computing\\n\\nNiobium Raises $23M+ to Advance Next-Gen FHE Hardware\\nQuantum National News\\n\\nNew Zealand Partners With Korea on Quantum Communication Projects\\n\\nUK And Germany Deepen Science And Tech Ties With £14 Million to Unlock Quantum’s Vast Potential\\n\\nChinese Research Team Launches Quantum Computing Platform Aimed at Speeding Scientific Work\\n\\nIndia’s Quantum Roadmap Targets 10 Globally Competitive Startups by 2035 in Bid to Become a Top-Three Power\\n\\nUK Backs First Mobile Quantum Brain Scanner to Study Blast Effects on Troops\\n\\nGuest Post: Forget the Qubits\\nQuantum Research News\\n\\nNew Zealand Partners With Korea on Quantum Communication Projects\\n\\na16z Researcher Calls for Measured Quantum Security Shift, Not Panic\\n\\nFirst Successful Proof Of Quantum Teleportation Between Two Different Quantum Dots\\n\\nJapan Brings Ion-Trap Qubits Online Through The Cloud in a Step Toward Remote Quantum Computing\\n\\nWhen Will Quantum Technologies Become Part of Everyday Life?\\n\\nChinese Research Team Launches Quantum Computing Platform Aimed at Speeding Scientific Work\\n\\nMarketing\\nWe create captivating digital marketing campaigns for the world’s leading Quantum Technology companies.\\nTry our\\nQuantum Market Intelligence Today\\n\\nStay Updated\\nJoin Our Newsletter\\nYou can unsubscribe anytime. For more details, review our Privacy Policy.\\n\\nFeatured News\\nXanadu Expands Partnership with ASTAR to Advance Photonic Quantum Computing\\nAutocrypt Announces Release of Post-Quantum PKI Product for Automotive OEMs\\nIonQ Expands in EU With Slovakia’s First National Quantum Communication Network\\nXanadu Expands Partnership with ASTAR to Advance Photonic Quantum Computing\\nAutocrypt Announces Release of Post-Quantum PKI Product for Automotive OEMs\\nIonQ Expands in EU With Slovakia’s First National Quantum Communication Network\\nNavigate\\nProjects\\nContact Us\\nLegal\\nOne of our team will be in touch to learn more about your requirements, and provide pricing and access options.\\nSubscribe to our industry leading leading newsletter for the latest in quantum news and insights.',\n",
    "#   \"Quantum Computers News\\nTop Headlines\\nLatest Headlines\\nEarlier Headlines\\nFriday, July 25, 2025\\nThursday, July 24, 2025\\nWednesday, July 2, 2025\\nWednesday, June 25, 2025\\nThursday, June 26, 2025\\nMonday, June 30, 2025\\nTuesday, June 10, 2025\\nSaturday, June 14, 2025\\nSunday, June 8, 2025\\nThursday, May 29, 2025\\nFriday, May 23, 2025\\nFriday, May 9, 2025\\nThursday, May 8, 2025\\nTuesday, May 6, 2025\\nMonday, May 5, 2025\\nThursday, May 1, 2025\\nWednesday, April 30, 2025\\nFriday, April 25, 2025\\nFriday, April 18, 2025\\nThursday, April 17, 2025\\nTuesday, April 15, 2025\\nMonday, April 14, 2025\\nThursday, April 10, 2025\\nMonday, April 7, 2025\\nWednesday, April 2, 2025\\nThursday, March 27, 2025\\nWednesday, March 26, 2025\\nTuesday, March 25, 2025\\nFriday, March 21, 2025\\nWednesday, March 12, 2025\\nTuesday, March 11, 2025\\nWednesday, March 5, 2025\\nTuesday, March 4, 2025\\nThursday, February 27, 2025\\nWednesday, February 26, 2025\\nTuesday, February 25, 2025\\nThursday, February 20, 2025\\nWednesday, February 19, 2025\\nFriday, February 14, 2025\\nTuesday, February 11, 2025\\nMonday, February 10, 2025\\nFriday, February 7, 2025\\nWednesday, February 5, 2025\\nTuesday, February 4, 2025\\nTuesday, January 28, 2025\\nMonday, January 27, 2025\\nFriday, January 24, 2025\\nThursday, January 23, 2025\\nTuesday, January 14, 2025\\nThursday, January 9, 2025\\nMonday, January 6, 2025\\nFriday, December 20, 2024\\nThursday, December 19, 2024\\nWednesday, December 11, 2024\\nTuesday, December 10, 2024\\nWednesday, November 27, 2024\\nThursday, November 21, 2024\\nWednesday, November 13, 2024\\nTuesday, November 12, 2024\\nMonday, November 11, 2024\\nTuesday, November 5, 2024\\nThursday, October 31, 2024\\nWednesday, October 30, 2024\\nMonday, October 28, 2024\\nThursday, October 24, 2024\\nWednesday, October 23, 2024\\nTuesday, October 22, 2024\\nMonday, October 21, 2024\\nThursday, October 17, 2024\\nWednesday, October 16, 2024\\nThursday, October 3, 2024\\nWednesday, September 18, 2024\\nMonday, September 16, 2024\\nThursday, September 12, 2024\\nWednesday, September 11, 2024\\nFriday, August 30, 2024\\nThursday, August 29, 2024\\nFriday, August 23, 2024\\nThursday, August 22, 2024\\nTuesday, August 20, 2024\\nBreaking\\nTrending Topics\\nStrange & Offbeat\\nStay informed with ScienceDaily's free email newsletter, updated daily and weekly. Or view our many newsfeeds in your RSS reader:\\nKeep up to date with the latest news from ScienceDaily via social networks:\\nTell us what you think of ScienceDaily -- we welcome both positive and negative comments. Have any problems using the site? Questions?\",\n",
    "#   'quantum computing\\n\\nWhy One VC Thinks Quantum Is a Bigger Unlock Than AGI\\n\\nA Special Diamond Is the Key to a Fully Open Source Quantum Sensor\\n\\nHow Supercomputing Will Evolve, According to Jack Dongarra\\n\\nSee How Much Faster a Quantum Computer Will Crack Encryption\\n\\nA New Quantum Algorithm Speeds Up Solving a Huge Class of Problems\\n\\nQuantum Computing Is Dead. Long Live Quantum Computing!\\n\\nThe Quantum Apocalypse Is Coming. Be Very Afraid\\n\\nMicrosoft’s New Majorana 1 Processor Could Transform Quantum Computing\\n\\nThe Incredible Power of Quantum Memory\\n\\nCryptographers Are Discovering New Rules for Quantum Encryption\\n\\nNever-Repeating Patterns of Tiles Can Safeguard Quantum Information\\n\\nApple’s iMessage Is Getting Future-Resistant Encryption\\n\\nThe Holy Grail of Quantum Computing Is Finally Here. Or Is It?\\n\\nThis Laser Can Help Verify the Source of a Diamond\\n\\nThe Quest to Use Quantum Mechanics to Pull Energy Out of Nothing\\n\\nHow Quantum Physicists ‘Flipped Time’ (and Didn’t)\\n\\nUS Technological Dominance Is Not What It Used to Be\\n\\nThe WIRED Guide to Quantum Computing\\n\\nQuantum Startups’ Stock Market Dreams Are Decohering\\n\\nQuantum Computing Has a Noise Problem\\n\\nIs Moore’s Law Really Dead?\\n\\nThree-Way Entanglement Results Hint at Better Quantum Codes\\n\\nA New Attack Easily Knocked Out a Potential Encryption Algorithm\\n\\nQuantum Advantage Showdowns Have No Clear Winners\\n\\n© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices\\nSelect international site',\n",
    "#   \"Suggestions or feedback?\\nMIT News | Massachusetts Institute of Technology - On Campus and Around the world\\nBrowse By\\nTopics\\nDepartments\\nCenters, Labs, & Programs\\nSchools\\nBreadcrumb\\nTopic\\nQuantum computing\\nDownload RSS feed: News Articles / In the Media / Audio\\n\\nQuantum modeling for breakthroughs in materials science and sustainable energy\\nQuantum chemist and School of Science Dean’s Postdoctoral Fellow Ernest Opoku is working on computational methods to study how electrons behave.\\nNovember 19, 2025\\nRead full story →\\n\\nFrom nanoscale to global scale: Advancing MIT’s special initiatives in manufacturing, health, and climate\\nMIT.nano cleanroom complex named after Robert Noyce PhD ’53 at the 2025 Nano Summit.\\nNovember 13, 2025\\nRead full story →\\n\\nLeading quantum at an inflection point\\nThe MIT Quantum Initiative is taking shape, leveraging quantum breakthroughs to drive the future of scientific and technological progress.\\nNovember 10, 2025\\nRead full story →\\n\\nStartup provides a nontechnical gateway to coding on quantum computers\\nCo-founded by Kanav Setia and Jason Necaise ’20, qBraid lets users access the most popular quantum devices and software programs on an intuitive, cloud-based platform.\\nNovember 4, 2025\\nRead full story →\\n\\nSolar energy startup Active Surfaces wins inaugural PITCH.nano competition\\nTwelve START.nano companies competed for the grand prize of nanoBucks to be used at MIT.nano’s facilities.\\nOctober 20, 2025\\nRead full story →\\n\\nWhy some quantum materials stall while others scale\\nIn a new study, MIT researchers evaluated quantum materials’ potential for scalable commercial success — and identified promising candidates.\\nOctober 15, 2025\\nRead full story →\\n\\nLincoln Laboratory technologies win seven R&D 100 Awards for 2025\\nInventions that protect US service members, advance computing, and enhance communications are recognized among the year's most significant new products.\\nSeptember 9, 2025\\nRead full story →\\n\\nTheory-guided strategy expands the scope of measurable quantum interactions\\nAn oft-ignored effect can be used to probe an important property of semiconductors, a new study finds.\\nJuly 24, 2025\\nRead full story →\\n\\nProfessor Emeritus Daniel Kleppner, highly influential atomic physicist, dies at 92\\nThe “godfather of Bose-Einstein condensation” and MIT faculty member for 37 years led research into atomic, molecular, and optical physics that led to GPS and quantum computing.\\nJuly 15, 2025\\nRead full story →\\n\\nThe high-tech wizardry of integrated photonics\\nPhD candidate Sabrina Corsetti builds photonic devices that manipulate light to enable previously unimaginable applications, like pocket-sized 3D printers.\\nJuly 2, 2025\\nRead full story →\\n\\nNew 3D chips could make electronics faster and more energy-efficient\\nThe low-cost, scalable technology can seamlessly integrate high-speed gallium nitride transistors onto a standard silicon chip.\\nJune 18, 2025\\nRead full story →\\n\\nClosing in on superconducting semiconductors\\nPlasma Science and Fusion Center researchers created a superconducting circuit that could one day replace semiconductor components in quantum and high-performance computing systems.\\nJune 17, 2025\\nRead full story →\\n\\nMIT engineers advance toward a fault-tolerant quantum computer\\nResearchers achieved a type of coupling between artificial atoms and photons that could enable readout and processing of quantum information in a few nanoseconds.\\nApril 30, 2025\\nRead full story →\\n\\nDevice enables direct communication among multiple quantum processors\\nMIT researchers developed a photon-shuttling “interconnect” that can facilitate remote entanglement, a key step toward a practical quantum computer.\\nMarch 21, 2025\\nRead full story →\\n\\nResearchers establish new basis for quantum sensing and communication\\nNew theoretical approach for generating quantum states could lead to improved accuracy and reliability of information and decision systems.\\nMarch 13, 2025\\nRead full story →\\nPagination\\nMore about MIT News at Massachusetts Institute of Technology\\nThis website is managed by the MIT News Office, part of the Institute Office of Communications.\\nNews by Schools/College:\\nResources:\\nTools:\\nMassachusetts Institute of Technology\\n77 Massachusetts Avenue, Cambridge, MA, USA\",\n",
    "#   'Bios\\n\\nPages\\n\\nContent\\n\\nNo matches\\nWe couldn\\'t find any matches for \"\".\\nDouble check your spelling or try a different search term.\\nStill can\\'t find what you\\'re looking for? Check out our featured articles.\\nFeatured Articles\\nDefining tokens\\nWhen to flip the fee switch\\nQuantum computing and blockchains: Matching urgency to actual threats\\nTABLE OF CONTENTS\\nTags\\nTimelines to a cryptographically relevant quantum computer are frequently overstated — leading to calls for urgent, wholesale transitions to post-quantum cryptography.\\nBut these calls often overlook the costs and risks of premature migration, and ignore the very different risk profiles of different cryptographic primitives:\\nThese distinctions matter. Misconceptions distort cost-benefit analyses, causing teams to overlook more salient security risks — like bugs.\\nThe real challenge in navigating a successful migration to post-quantum cryptography is matching urgency to actual threats. Below, I clarify common misconceptions about quantum threats to cryptography — covering encryption, signatures, and zero-knowledge proofs — with a special focus on their implications for blockchains.\\nWhere are we on timing?\\nA cryptographically relevant quantum computer (CRQC) in the 2020s is highly unlikely, despite high-profile claims otherwise.\\nBy a “cryptographically relevant quantum computer” I mean a fault-tolerant, error-corrected quantum computer capable of running Shor’s algorithm at scales sufficient to attack elliptic curve cryptography or RSA within a reasonable timeframe (e.g., breaking secp256k1 or RSA-2048 with at most, say, one month of sustained computation).\\nWe are nowhere near a cryptographically relevant quantum computer by any reasonable reading of public milestones and resource estimates. Companies sometimes claim a CRQC is likely before 2030 or well before 2035, but publicly known progress doesn’t support those claims.\\nFor context, across all current architectures — trapped ions, superconducting qubits, and neutral atom systems — no quantum computing platform today comes close to the hundreds of thousands to millions of physical qubits (depending on error rates and error-correction schemes) required to run Shor’s algorithm on RSA-2048 or secp256k1.\\nThe limiting factor is not just qubit count, but gate fidelities, qubit connectivity, and the sustained error-corrected circuit depth needed to run deep quantum algorithms. While some systems now exceed 1,000 physical qubits, raw qubit count alone is misleading: These systems lack the qubit connectivity and gate fidelities needed for cryptographically relevant computation.\\nRecent systems approach the physical error rates where quantum error correction begins to work, but no one has demonstrated more than a handful of logical qubits with sustained error-corrected circuit depth… much less the thousands of high-fidelity, deep-circuit, fault-tolerant logical qubits actually required to run Shor’s algorithm. The gap between demonstrating that quantum error correction works in principle, and achieving the scale needed for cryptanalysis, remains vast.\\nIn short: Until both qubit numbers and fidelities improve by several orders of magnitude, a cryptographically relevant quantum computer remains far beyond reach.\\nIt’s easy to get confused by corporate press releases and media coverage, however. Some common misconceptions and sources of confusion here include:\\nDemos claiming “quantum advantage”, which currently target contrived tasks. These tasks aren’t selected for their practical usefulness, but because they can run on existing hardware while appearing to exhibit large quantum speedups — a fact that is often obscured in announcements.\\nCompanies claiming to have achieved many thousands of physical qubits. But this refers to quantum annealers, not the gate-model machines needed to run Shor’s algorithm to attack public-key cryptography.\\nCompanies making liberal use of the term “logical qubit”. Physical qubits are noisy. Quantum algorithms need logical qubits, as mentioned above; Shor’s algorithm requires thousands of them. Using quantum error-correction, one can implement a logical qubit out of many physical qubits — typically hundreds to thousands depending on error rates. But some companies have stretched the term beyond recognition. For example, one recent announcement claimed to have achieved 48 logical qubits using a distance-2 code with only two physical qubits per logical qubit. This is absurd: distance-2 codes can only detect errors, not correct them. Real fault-tolerant logical qubits for cryptanalysis require hundreds to thousands of physical qubits each, not two.\\nMore generally, many quantum computing roadmaps use the term “logical qubit” to refer to qubits that support only Clifford operations. These operations are efficiently classically simulable, and therefore insufficient to run Shor’s algorithm, which needs thousands of error-corrected T gates (or non-Clifford gates more generally).\\nEven if one of those roadmaps aims for “thousands of logical qubits by year X”, that does not mean the company expects to run Shor’s algorithm to break classical cryptography by that same year X.\\nThese practices have seriously distorted public perception of how close we are to a cryptographically relevant quantum computer, even among sophisticated observers.\\nThat said, some experts are indeed excited by progress. Scott Aaronson for instance recently wrote that, given the “current staggering rate of hardware progress”,\\nI now think it’s a live possibility that we’ll have a fault-tolerant quantum computer running Shor’s algorithm before the next U.S. presidential election.\\nBut Aaronson later clarified that his statement does not mean a cryptographically relevant quantum computer: He’d count it as fulfilled even if a fully fault-tolerant run of Shor’s algorithm factored 15 = 3×5 — a calculation you could do faster with pencil and paper. The bar is still a tiny-scale execution of Shor’s algorithm, not a cryptographically relevant one, as previous factorings of 15 on quantum computers used simplified circuits rather than full, fault-tolerant Shor. And there’s a reason these experiments consistently target 15 as the number to factor: Arithmetic modulo 15 is computationally easy, whereas factoring even slightly larger numbers like 21 is far harder. Consequently, quantum experiments claiming to factor 21 typically rely on additional hints or shortcuts.\\nSimply put, the expectation of a cryptographically relevant quantum computer capable of breaking RSA-2048 or secp256k1 in the next 5 years — which is what matters for practical cryptography — is unsupported by publicly known progress.\\nEven 10 years remains ambitious. Given how far away we are from a cryptographically relevant quantum computer, excitement about progress is entirely compatible with a decade-plus timeline.\\nWhat about the U.S. government’s targeting 2035 as a deadline for wholesale post-quantum (PQ) migration of government systems? I consider this a reasonable timeline for completing such a large-scale transition. However, it is not a forecast that a cryptographically relevant quantum computer will exist by then.\\nWhere do HNDL attacks apply (and where not)?\\nHarvest now, decrypt later (HNDL) attacks refer to adversaries storing encrypted traffic now, and then decrypting it later when a cryptographically relevant quantum computer exists. Nation-state level adversaries are surely already archiving encrypted communications at scale from the United States government, so they can decrypt these communications many years from now when a CRQC does exist.\\nThat’s why encryption needs to transition today — at least for anyone with 10-50+ year confidentiality needs.\\nBut digital signatures — which all blockchains rely on — are different from encryption: There’s no confidentiality to retroactively attack.\\nIn other words, if a cryptographically relevant quantum computer arrives, signature forgery does become possible from that point forward, but past signatures weren’t “hiding” secrets the way that encrypted messages are. As long as you know the digital signature was generated before a CRQC arrived, it cannot be a forgery.\\nThis makes the transition to post-quantum digital signatures less urgent than the post-quantum transition for encryption.\\nMajor platforms are acting accordingly: Chrome and Cloudflare rolled out hybrid X25519+ML-KEM for web transport-layer security (TLS) encryption. [Throughout this post, I refer to encryption schemes for readability, though strictly speaking, secure communication protocols like TLS use key exchange or key encapsulation mechanisms rather than public-key encryption.]\\n“Hybrid” here means using both a post-quantum-secure scheme (namely ML-KEM) and an existing scheme (X25519) on top of each other, to get the combined security guarantees of both. This way they can (hopefully) stymie HNDL attacks via ML-KEM, while maintaining classical security from X25519 in the event that ML-KEM turns out to be insecure even against today’s computers.\\nApple’s iMessage also deployed such hybrid post-quantum encryption with its PQ3 protocol, as did Signal with its PQXDH and SPQR protocols.\\nBy contrast, the rollout of post-quantum digital signatures to critical web infrastructure is being delayed until a cryptographically relevant quantum computer is actually imminent, because current post-quantum signature schemes introduce performance regressions (more on that later in this post).\\nzkSNARKs — zero-knowledge Succinct Non-interactive ARguments of Knowledge, which are key to the long term scalability and privacy of blockchains — occupy a similar situation to signatures. This is because even for zkSNARKs that are not post-quantum-secure (they use elliptic curve cryptography, just like today’s non-post-quantum encryption and signature schemes), their zero-knowledge property is post-quantum secure.\\nThe zero-knowledge property ensures that nothing about the secret witness is revealed in the proof — not even to a quantum adversary — so there is no confidential information to “harvest now” for later decryption.\\nHence, zkSNARKs are not vulnerable to harvest now, decrypt later attacks. Just as a non-post-quantum signature generated today is secure, any zkSNARK proof that was generated before a cryptographically relevant quantum computer arrived is trustworthy (that is, the statement being proved is definitely true) — even if the zkSNARK uses elliptic curve cryptography. Only after a cryptographically relevant quantum computer arrives can attackers find convincing proofs of false statements.\\nWhat this all means for blockchains\\nMost blockchains are not exposed to HNDL attacks:\\nMost non-privacy chains, like Bitcoin and Ethereum today, use non-post-quantum cryptography mainly for transaction authorization — that is, they use digital signatures, not encryption.\\nAgain, those signatures are not an HNDL risk: “Harvest now, decrypt later” attacks apply to encrypted data. For example, Bitcoin’s blockchain is public; the quantum threat is signature forgery (deriving private keys to steal funds), not decrypting already-public transaction data. This eliminates the immediate cryptographic urgency from HNDL attacks.\\nUnfortunately, even analyses from credible sources like the Federal Reserve incorrectly claim that Bitcoin is vulnerable to HNDL attacks, a mistake that exaggerates the urgency of transitioning to post-quantum cryptography.\\nThat said, reduced urgency doesn’t mean that Bitcoin can wait: It faces different timeline pressures from the immense social coordination required to change the protocol. (More on Bitcoin’s unique challenges below.)\\nThe exception as of today is privacy chains, many of which encrypt or otherwise hide recipients and amounts. That confidentiality can be harvested now and retroactively deanonymized once a quantum computer can break elliptic-curve cryptography.\\nFor such privacy chains, the severity of attack varies by blockchain design. For example, with Monero’s curve-based ring signatures and key images (a per-output linkability tag used to stop double-spends), the public ledger alone would largely suffice for retroactively reconstructing the spend-graph. But in others the damage is more limited — see Zcash cryptographic engineer and researcher Sean Bowe’s discussion for details.\\nIf it’s important to users that their transactions not be exposed by a cryptographically relevant quantum computer, then privacy chains should transition to post-quantum primitives (or hybrids) as soon as feasible. Or, they should adopt architectures that avoid placing decryptable secrets on-chain.\\nBitcoin’s special headaches: governance + abandoned coins\\nFor Bitcoin especially, two realities drive the urgency to begin switching to post-quantum digital signatures. Neither has anything to do with quantum technology.\\nOne concern is governance speed: Bitcoin changes slowly. Any contentious issues could trigger a damaging hard fork if the community cannot agree on the appropriate solution.\\nAnother concern is that Bitcoin’s switch to post-quantum signatures cannot be a passive migration: Owners must actively migrate their coins. This means abandoned, quantum-vulnerable coins cannot be protected. Some estimates place the amount of quantum-vulnerable and potentially abandoned BTC in the millions of coins, worth hundreds of billions of dollars at current prices (as of December 2025).\\nHowever, the quantum threat to Bitcoin won’t be a sudden, overnight apocalypse… but more like a selective, progressive targeting process. Quantum computers won’t break all encryption simultaneously — Shor’s algorithm must target individual public keys one at a time. Early quantum attacks will be extremely expensive and slow. So once quantum computers are able to crack a single Bitcoin signing key, attackers will selectively prey on high-value wallets.\\nMoreover, users who avoid address reuse and don’t use Taproot addresses — which expose public keys directly on-chain — are largely protected even without protocol changes: Their public keys remain hidden behind hash functions until their coins are spent. When they finally broadcast a spending transaction, the public key becomes visible and there’s a short real-time race, between the honest spender who needs to get their transaction confirmed, and any quantum-equipped attacker who wants to find the private key and spend the coins before the real owner’s transaction is final. So the truly vulnerable coins are those with public keys already exposed: early P2PK outputs, reused addresses, and Taproot holdings.\\nFor vulnerable coins that have been abandoned, there is no easy solution. Some options include:\\nThe second option creates serious legal and security problems. Using a quantum computer to take possession of coins without the private key — even with claimed, legitimate ownership or good intentions — could raise serious issues in many jurisdictions under theft and computer fraud laws.\\nFurthermore, “abandoned” is itself a presumption based on inactivity. But no one actually knows whether these coins lack a living owner with access to the keys. Evidence that you once owned coins may not provide sufficient legal authority to break cryptographic protections to reclaim them. This legal ambiguity increases the likelihood that abandoned quantum-vulnerable coins fall into the hands of malicious actors willing to ignore legal constraints.\\nA final issue specific to Bitcoin is its low transaction throughput. Even once migration plans are finalized, migrating all quantum-vulnerable funds to post-quantum-secure addresses would take months at Bitcoin’s current transaction rate.\\nThese challenges make it critical for Bitcoin to begin planning its post-quantum transition now — not because a cryptographically relevant quantum computer is likely before 2030, but because the governance, coordination, and technical logistics of migrating billions of dollars worth of coins will take years to resolve.\\nThe quantum threat to Bitcoin is real, but the timeline pressure comes from Bitcoin’s own constraints, not from imminent quantum computers. Other blockchains face their own challenges with quantum-vulnerable funds, but Bitcoin is uniquely exposed: Its earliest transactions used pay-to-public-key (P2PK) outputs that place public keys directly on-chain, leaving an especially significant fraction of BTC vulnerable to cryptographically relevant quantum computers. This technical difference — combined with Bitcoin’s age, value concentration, low throughput, and governance rigidity — makes the problem especially severe.\\nNote that the vulnerabilities I describe above apply to the cryptographic security of Bitcoin’s digital signatures — but not to the economic security of the Bitcoin blockchain. This economic security derives from the proof-of-work (PoW) consensus mechanism, which is not as vulnerable to attacks from quantum computers for three reasons:\\nThe costs and risks of post-quantum signatures\\nTo see why blockchains shouldn’t rush post-quantum signature deployment, we need to understand both the performance costs and our still-evolving confidence in post-quantum security.\\nMost post-quantum cryptography is based on one of five approaches:\\nWhy are there five different approaches? The security of any post-quantum cryptographic primitive rests on the assumption that quantum computers cannot efficiently solve a specific mathematical problem. The more “structured” that problem is, the more efficient the cryptographic protocols we can build from it.\\nBut this cuts both ways: Additional structure also creates more surface area for attack algorithms to exploit. This creates a fundamental tension — stronger assumptions enable better performance, but at the cost of potential security vulnerabilities (that is, increased likelihood that the assumptions turn out to be wrong).\\nGenerally speaking, hash-based approaches are the most conservative security-wise, since we have the most confidence that quantum computers cannot efficiently attack these protocols. But they are also the least performant. For example, hash-based signatures standardized by NIST have size 7-8 kilobytes even at its smallest parameter settings. For comparison, today’s elliptic-curve-based digital signatures are only 64 bytes. This is roughly a 100x difference in size.\\nLattice schemes are a major focus for deployment today. The only encryption scheme, and two of the three signature algorithms already selected by NIST for standardization, are based on lattices. One lattice scheme (ML-DSA, formerly Dilithium) produces signatures ranging from 2.4 KB (at the 128-bit security level) to 4.6 KB (at the 256-bit security level) — making them roughly 40-70x bigger than today’s elliptic curve-based ones. The other lattice scheme, Falcon, has somewhat smaller signatures (666 bytes for Falcon-512 and 1.3 KB for Falcon-1024) but comes with complex floating-point arithmetic that NIST itself flags as a special implementation challenge. One of the creators of Falcon, Thomas Pornin, called it “by far the most complicated cryptographic algorithm I have ever implemented.”\\nImplementation security is also much more challenging with lattice-based than elliptic-curve-based signature schemes: ML-DSA has many more sensitive intermediate values and nontrivial rejection-sampling logic that needs side-channel and fault protection. Falcon adds constant-time floating-point concerns; several side-channel attacks on Falcon implementations have in fact recovered secret keys.\\nThese issues pose immediate risks, unlike the much more distant threat of cryptographically relevant quantum computers.\\nThere’s good reason to be cautious when deploying more performant approaches to post-quantum cryptography. Historically, leading candidates like Rainbow (an MQ-based signature scheme) and SIKE/SIDH (an isogeny-based encryption scheme) were broken classically, that is, broken using today’s computers, not quantum ones.\\nThis happened well into NIST’s standardization process. That’s healthy science doing its job, but it illustrates that premature standardization and deployment can backfire.\\nAs mentioned earlier, internet infrastructure is taking a deliberate approach to signature migration. This is especially notable given how long cryptographic transitions for the internet actually take once begun. The move away from MD5 and SHA-1 hash functions — technically deprecated by web-governing bodies years ago — took many more years to actually implement across infrastructure, and is still ongoing in some contexts. This happened despite those schemes being completely broken, not just being potentially vulnerable to future technology.\\nChallenges unique to blockchain vs. internet infrastructure\\nFortunately, blockchains that are actively maintained by communities of open source developers — like Ethereum or Solana — can upgrade more quickly than traditional web infrastructure. On the other hand, traditional web infrastructure benefits from frequent key rotation, which means its attack surface moves faster than early quantum machines could target — a luxury blockchains don’t have, since coins and their associated keys can sit exposed indefinitely.\\nBut on balance, blockchains should still follow the web’s deliberate approach to signature migration. Neither setting is exposed to HNDL attacks for signatures, and the costs and risks of prematurely migrating to immature post-quantum schemes remain significant regardless of how long keys persist.\\nThere are also challenges specific to blockchains that make premature migration especially risky and complex: For example, blockchains have unique requirements for signature schemes, particularly the ability to quickly aggregate many signatures. Today, BLS signatures are commonly used because they enable very fast aggregation, but they are not post-quantum secure. Researchers are exploring SNARK-based aggregation of post-quantum signatures. This work is promising, but still early.\\nFor SNARKs specifically, the community currently focuses on hash-based constructions as the leading post-quantum option. But a major shift is coming: I am confident that in the coming months and years, lattice-based options will emerge as attractive alternatives. These alternatives will have better performance in various respects than hash-based SNARKs, such as substantially shorter proofs — analogous to how lattice-based signatures are shorter than hash-based ones.\\nThe bigger problem right now: Implementation security\\nFor years to come, implementation vulnerabilities will be a far bigger security risk than a cryptographically relevant quantum computer. For SNARKs, the primary concern is bugs.\\nBugs are already a challenge for digital signature and encryption schemes, and SNARKs are vastly more complicated. Indeed, a digital signature scheme can be viewed as a very simple kind of zkSNARK, for the statement “I know the private key corresponding to my public key, and I authorized this message.”\\nFor post-quantum signatures, the immediate risks also include implementation attacks such as side-channel and fault-injection attacks. These kinds of attacks are well-documented and can extract secret keys from deployed systems. They pose far more pressing threats than do distant quantum computers.\\nThe community will be working for years to identify and fix bugs in SNARKs, and to harden post-quantum signature implementations against side-channel and fault-injection attacks. Since the dust has yet to settle around post-quantum SNARKs and signature aggregation schemes, blockchains that transition prematurely risk locking themselves into suboptimal schemes. They could need to migrate again when better options emerge, or when implementation vulnerabilities are discovered.\\nWhat should we do? 7 recommendations\\nGiven the realities I outline above, I’ll conclude with recommendations for various stakeholders — from builders to policymakers. The overarching principle: Take the quantum threat seriously, but do not act under the presumption that a cryptographically relevant quantum computer will arrive before 2030. This presumption is not justified by current progress. Nonetheless, there are still things we can and should do now:\\n#1 We should deploy hybrid encryption immediately.\\nOr at least, wherever long-term confidentiality matters and costs are tolerable.\\nMany browsers, CDNs, and messaging apps (like iMessage and Signal) already have deployed hybrid approaches. The hybrid approach — post-quantum + classical — protects against HNDL attacks while hedging against potential weaknesses in post-quantum schemes.\\n#2 Use hash-based signatures immediately when their large size is tolerable.\\nSoftware/ firmware updates — and other such low-frequency, size-insensitive contexts — should adopt hybrid hash-based signatures now. (Hybrid to hedge against implementation bugs in the new schemes, not because hash-based security assumptions are in doubt.)\\nThis is conservative and gives society a clear “lifeboat” in the unlikely event that a cryptographically relevant quantum computer appears unexpectedly soon. Without post-quantum-signed software updates already in place, we’d face a bootstrapping problem after a CRQC emerges: We wouldn’t be able to securely distribute the post-quantum cryptography fixes we’d need to withstand it.\\n#3 Blockchains don’t need to rush post-quantum signatures — but should start planning now.\\nBlockchain developers should follow the web PKI community’s lead in taking a deliberate approach to post-quantum signature deployment. This allows post-quantum signature schemes to continue maturing in both performance and our understanding of their security. This approach also allows developers time to re-architect systems to handle larger signatures and develop better aggregation techniques.\\nFor Bitcoin and other L1s: The community needs to define migration paths and policies on abandoned quantum-vulnerable funds. Passive migration is impossible, so planning is critical. And since Bitcoin faces special challenges that are mostly non-technical — slow governance, and a large number of high-value potentially abandoned quantum-vulnerable addresses — it’s especially important that the Bitcoin community begin that planning now.\\nMeanwhile, we need to allow research on post-quantum SNARKs and aggregatable signatures to mature (likely another couple of years). Again, migrating prematurely risks locking into suboptimal schemes or needing a second migration to address implementation bugs.\\nA note on Ethereum’s account models: Ethereum supports two account types with different implications for post-quantum migration — externally owned accounts (EOAs), the traditional account type controlled by secp256k1 private keys; and smart contract wallets with programmable authorization logic.\\nIn a non-emergency scenario where Ethereum adds post-quantum signature support, upgradeable smart contract wallets could switch to post-quantum verification via a contract upgrade — while EOAs would likely need their funds moved to new post-quantum-secure addresses (though Ethereum may well provide dedicated migration mechanisms for EOAs as well). In a quantum emergency, Ethereum researchers have proposed a hard-fork plan to freeze vulnerable accounts and let users recover funds by proving knowledge of their seed phrase using post-quantum-secure SNARKs. This recovery mechanism would apply to both EOAs and any smart contract wallets that hadn’t already been upgraded.\\nThe practical implication for users: Well-audited, upgradeable, smart contract wallets may provide a marginally smoother migration path — but the difference is modest and comes with tradeoffs around trust in wallet providers and upgrade governance. What matters more than account type is that the Ethereum community continues its work on post-quantum primitives and emergency-response plans.\\nA broader design lesson for builders: Many blockchains today tightly couple account identity to specific cryptographic primitives — Bitcoin and Ethereum to ECDSA signatures over secp256k1, others to EdDSA. The challenge of post-quantum migration highlights the value of decoupling account identity from any particular signature scheme. Ethereum’s move toward smart accounts and similar account-abstraction efforts on other chains reflect this trend: letting accounts upgrade their authentication logic without abandoning their on-chain history and state. This decoupling won’t make post-quantum migration trivial, but it does provide substantially more flexibility than hard-wiring accounts to a single signature scheme. (This also enables unrelated features like sponsored transactions, social recovery, and multisigs).\\n#4 For privacy chains, which encrypt or hide transaction details, prioritize a transition sooner if performance is tolerable.\\nUser confidentiality on these chains is currently exposed to HNDL attacks, though severity varies amongst designs. Chains where the public ledger alone enables full retroactive deanonymization face the most urgent risk.\\nConsider hybrid (post-quantum + classical) schemes to protect against ostensibly post-quantum schemes turning out to be even classically insecure, or implement architectural changes that avoid placing decryptable secrets on-chain.\\n#5 Prioritize implementation security — not quantum threat mitigation — in the near term.\\nEspecially for complex cryptographic primitives like SNARKs and post-quantum signatures, bugs and implementation attacks (side-channel attacks, fault injection) will be far bigger security risks than cryptographically relevant quantum computers for years to come.\\nInvest in auditing, fuzzing, formal verification, and defense in depth/layered security approaches right now — don’t let quantum worries overshadow the far more pressing threat of bugs!\\n#6 Fund quantum computing development.\\nA big national security implication of all of the above is that we need to sustain funding and talent development for quantum computing.\\nA major adversary achieving cryptographically relevant quantum computing capabilities before the U.S. does would pose severe national security risks to us and others around the world.\\n#7 Maintain perspective on quantum computing announcements.\\nThere will be many milestones in the years to come as quantum hardware matures. Paradoxically, the very frequency of these announcements is itself evidence of how far we remain from a cryptographically relevant quantum computer: Each milestone represents one of many bridges we must cross before reaching that point, and each will generate its own wave of headlines and excitement.\\nTreat press releases as progress reports to critically assess, not prompts for abrupt action.\\n\\nOf course, there can be surprising developments or innovations that accelerate projected timelines, just as there can be severe scaling bottlenecks that lengthen them.\\nI won’t argue that a cryptographically relevant quantum computer in five years is literally impossible, only highly unlikely. The recommendations above are robust to that uncertainty, and following them avoids the more immediate, more probable risks: implementation bugs, rushed deployments, and the ordinary ways cryptographic transitions go wrong.\\nJustin Thaler is Research Partner at a16z and an Associate Professor in the Department of Computer Science at Georgetown University. His research interests include verifiable computing, complexity theory, and algorithms for massive data sets.\\n—\\nThe views expressed here are those of the individual AH Capital Management, L.L.C. (“a16z”) personnel quoted and are not the views of a16z or its affiliates. Certain information contained in here has been obtained from third-party sources, including from portfolio companies of funds managed by a16z. While taken from sources believed to be reliable, a16z has not independently verified such information and makes no representations about the current or enduring accuracy of the information or its appropriateness for a given situation. In addition, this content may include third-party advertisements; a16z has not reviewed such advertisements and does not endorse any advertising content contained therein.\\nThe views expressed here are those of the individual AH Capital Management, L.L.C. (“a16z”) personnel quoted and are not the views of a16z or its affiliates. Certain information contained in here has been obtained from third-party sources, including from portfolio companies of funds managed by a16z. While taken from sources believed to be reliable, a16z has not independently verified such information and makes no representations about the current or enduring accuracy of the information or its appropriateness for a given situation. In addition, this content may include third-party advertisements; a16z has not reviewed such advertisements and does not endorse any advertising content contained therein.\\nYou should consult your own advisers as to those matters. References to any securities or digital assets are for illustrative purposes only, and do not constitute an investment recommendation or offer to provide investment advisory services. Furthermore, this content is not directed at nor intended for use by any investors or prospective investors, and may not under any circumstances be relied upon when making a decision to invest in any fund managed by a16z. (An offering to invest in an a16z fund will be made only by the private placement memorandum, subscription agreement, and other relevant documentation of any such fund and should be read in their entirety.) Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by a16z, and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results. A list of investments made by funds managed by Andreessen Horowitz (excluding investments for which the issuer has not provided permission for a16z to disclose publicly as well as unannounced investments in publicly traded digital assets) is available at \\nThe content speaks only as of the date indicated. Any projections, estimates, forecasts, targets, prospects, and/or opinions expressed in these materials are subject to change without notice and may differ or be contrary to opinions expressed by others. Please see  for additional important information.'],\n",
    "#  'referred_urls': ['https://thequantuminsider.com/',\n",
    "#   'https://www.sciencedaily.com/news/computers_math/quantum_computers/',\n",
    "#   'https://www.wired.com/tag/quantum-computing/',\n",
    "#   'https://news.mit.edu/topic/quantum-computing',\n",
    "#   'https://a16zcrypto.com/posts/article/quantum-computing-misconceptions-realities-blockchains-planning-migrations/']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cbc1a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm_gpt_5_mini = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "\n",
    "def write_initial_draft(state):\n",
    "    prompt = f\"\"\"Based on these articles (presented to you as a List[str]), write an initial draft of the research report with a suitable title based on the user query.\n",
    "User Query: {state['query']}\n",
    "List of articles: {state['tavily_responses']} \n",
    "\n",
    "The tone for the draft should be conversational as you explain the findings from input articles in layman terms.\n",
    "Target audience for this draft is: novice students (high school and undergrad)\n",
    "The draft should be about 800-1000 words (excluding HTML tags).\n",
    "\n",
    "Strictly Follow Below Rules:  \n",
    "- Do not miss out any detail/fact/figure from the articles passed to you for reference.\n",
    "- Don't return any additional opening or closing commentary of yours (only return the title and article draft)\n",
    "- Think step by step while generating the draft to deliver your finest work. Don't rush.\n",
    "- **Generate the report in HTML format**\n",
    "\"\"\"\n",
    "\n",
    "    response = llm_gpt_5_mini.invoke(prompt)\n",
    "    return {\"query\": state['query'], \"tavily_responses\": state['tavily_responses'], \"initial_draft\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5da6bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing write_initial_draft()\n",
    "\n",
    "# # state = {'query': \"latest updates on quantum computing\", 'tavily_responses': [\"News on quantum computing\\nQuantum computer\\nA quantum computer is a device for computation that makes direct use of quantum mechanical phenomena, such as superposition and entanglement, to perform operations on data. The basic principle behind quantum computation is that quantum properties can be used to represent data and perform operations on these data.\\nAlthough quantum computing is still in its infancy, experiments have been carried out in which quantum computational operations were executed on a very small number of qubits (quantum binary digits). Both practical and theoretical research continues with interest, and many national government and military funding agencies support quantum computing research to develop quantum computers for both civilian and national security purposes, such as cryptanalysis.\\nIf large-scale quantum computers can be built, they will be able to solve certain problems much faster than any of our current classical computers (for example Shor's algorithm). Quantum computers are different from other computers such as DNA computers and traditional computers based on transistors. Some computing architectures such as optical computers may use classical superposition of electromagnetic waves. Without some specifically quantum mechanical resources such as entanglement, it is conjectured that an exponential advantage over classical computers is not possible.\\nThis text uses material from Wikipedia,\\nlicensed under CC BY-SA\\n\\nTerahertz device sets performance record and opens new quantum horizons\\n\\nScientists advance quantum signaling with twisted light technology\\n\\nSynchrotron radiation sources: Toolboxes for quantum technologies\\n\\nCan quantum computers help researchers learn about the inside of a neutron star?\\n\\nCorral technique measures fragile quantum states in magnet-superconductor hybrids from afar\\n\\nShop-bought cable helps power two quantum networks\\n\\nSymmetry simplifies quantum noise analysis, paving way for better error correction\\n\\nBright squeezed vacuum reveals hidden quantum effects in strong-field physics\\n\\nEfficient quantum process tomography for enabling scalable optical quantum computing\\n\\nControlling triple quantum dots in a zinc oxide semiconductor\\nAbout\\nPhys.org™ is a leading web-based science, research and technology news service which covers a full range of topics.\\nPhys.org is a part of Science X network.\\nWith a global reach of over 10 million monthly readers and featuring dedicated websites for science (Phys.org),\\ntechnology (Tech Xplore) and medical research (Medical Xpress),\\nthe Science X network is one of the largest online communities for science-minded people.\\nScience X Account\\nForgot Password?\\nNot a member? Sign up.\\nIdentify the news topics you want to see and prioritize an order.\\nScience X Daily and the Weekly Email Newsletter are free features that allow you to receive your favorite sci-tech news updates in your email inbox\\nGet in touch\\nOur products\\nOther publications\\nExtras\\nLegal\\n© Phys.org 2003 - 2025 powered by Science X Network\\nScience news, straight to your inbox\\nYour favorite sci-tech news, delivered for free with Science X Daily and Weekly email newsletters.\\nSubscribe now and get a confirmation link to customize your preferences!\\nScience never stops. Get notified about trending stories.\",\n",
    "# #   'Quantum Insider Intelligence\\nAccelerate your quantum strategy\\nGain a 360-degree view of the quantum landscape\\u2028 to fuel your strategy\\n\\nTrusted by:\\n\\nQuantum Computing News\\n\\nAutocrypt Announces Release of Post-Quantum PKI Product for Automotive OEMs\\n\\nIonQ Expands in EU With Slovakia’s First National Quantum Communication Network\\n\\nQuantum Source Report Outlines Engineering Pathways to Fault-Tolerant Quantum Computing\\n\\nNew Zealand Partners With Korea on Quantum Communication Projects\\n\\nWhat Is The Price Of A Quantum Computer In 2025?\\nFeatured Articles\\n\\nQuantum Source Report Outlines Engineering Pathways to Fault-Tolerant Quantum Computing\\n\\nQuantum Computing Companies in 2025: Mapping the Global Quantum Landscape\\n\\nDeterministic Entanglement for Photonic Quantum Computing: Atom as Interface\\n\\nWQD 25: 12 Events For World Quantum Day 2025\\n\\nQuantum Source’s Scalable Photon-Atom Technology Enables Practical Quantum Computing\\n\\nKeep track of the Quantum Technology Market.\\nIn one place.\\nQuantum Business News\\u200b\\n\\nIonQ Expands in EU With Slovakia’s First National Quantum Communication Network\\n\\nQuantum Source Report Outlines Engineering Pathways to Fault-Tolerant Quantum Computing\\n\\nAutocrypt Announces Release of Post-Quantum PKI Product for Automotive OEMs\\nInsider Brief PRESS RELEASE — Leading automotive cybersecurity solutions provider Autocrypt announced on December 8, 2025, the launch of “AutoCrypt PKI-Vehicles,” a new next-generation Public Key\\n\\nWhat Is The Price Of A Quantum Computer In 2025?\\n\\nSergio Gago, CTO of Cloudera and former Head of AI & Quantum at Moody’s\\nQuantum Computing Exclusives\\n\\nQuantum World Tour: Brazil Demonstrates National Vision Fueling An Emerging Quantum Ecosystem\\nInsider Brief: The Quantum World Tour, a global initiative launched by the International Telecommunication Union (ITU) in partnership with The Quantum Insider, will host its\\nQuantum World Tour Saudi Arabia: Inside the Kingdom’s Quantum Expansion\\nQuantum World Tour: Brazil Demonstrates National Vision Fueling An Emerging Quantum Ecosystem\\nTelecom at the Edge of Scale: How Quantum Technologies Are Recasting the Network Economy\\nDefence Panel Discusses How to Bring Quantum & AI to Bear in a Hostile World\\nQuantum World Tour Heads to Australia, Showcasing Strategy, Science, and Emerging Industry Strengths\\n\\nQuantum World Tour Debuts in Malta, Europe’s Emerging Quantum Hub\\n\\nThe Quantum Economy Podcast: Audacity in EU Quantum with Théau Peronnin and Anders Indset\\n\\nQuantum Marketing Solutions\\nDigital marketing campaigns for the world’s leading Quantum Technology companies.\\n\\nQuantum Industry Intelligence\\nThe leading provider of information, data, and insights on Quantum Technologies.\\nQuantum Interviews\\n\\nTQI Exclusive: Microsoft’s Krysta Svore Says Geometric Error-Correcting Codes Are a Step Toward Practical Applications\\n\\nWhat’s Quantum Biology? A Research Pioneer Shares His Vision for Quantum Technology’s Next Frontier\\n\\nStartup Fortaegis Says Its Hardware is Ready For Today’s Security Challenges – Perfectly Positioned For Tomorrow’s Quantum Era\\n\\nTQI Exclusive: Main Sequence’s Latest Thesis Guides Quantum Bets on the Edge of Commercial Use\\n\\nQuantum Source’s Scalable Photon-Atom Technology Enables Practical Quantum Computing\\n\\nSony Ventures Looks to Quantum and Renewable Energy for Scalable Deep Tech Investments\\nCapital Markets News\\n\\nHorizon Quantum Secures $110 Million PIPE, With IonQ Among Lead Investors, to Support SPAC Merger\\n\\nSEALSQ Makes Strategic Investment in EeroQ\\n\\nParityQC Awarded Contract by DLR to Integrate Quantum Computing for Next-Generation Mobility Solutions\\n\\nNTT Focuses on Light For Cleaner, Scalable Path to Quantum Computing\\n\\nNiobium Raises $23M+ to Advance Next-Gen FHE Hardware\\n\\nDelft Circuits Names Martin Danoesastro CEO and Extends Funding Round\\nQuantum National News\\n\\nNew Zealand Partners With Korea on Quantum Communication Projects\\n\\nUK And Germany Deepen Science And Tech Ties With £14 Million to Unlock Quantum’s Vast Potential\\n\\nChinese Research Team Launches Quantum Computing Platform Aimed at Speeding Scientific Work\\n\\nIndia’s Quantum Roadmap Targets 10 Globally Competitive Startups by 2035 in Bid to Become a Top-Three Power\\n\\nUK Backs First Mobile Quantum Brain Scanner to Study Blast Effects on Troops\\n\\nGuest Post: Forget the Qubits\\nQuantum Research News\\n\\nNew Zealand Partners With Korea on Quantum Communication Projects\\n\\na16z Researcher Calls for Measured Quantum Security Shift, Not Panic\\n\\nFirst Successful Proof Of Quantum Teleportation Between Two Different Quantum Dots\\n\\nJapan Brings Ion-Trap Qubits Online Through The Cloud in a Step Toward Remote Quantum Computing\\n\\nWhen Will Quantum Technologies Become Part of Everyday Life?\\n\\nChinese Research Team Launches Quantum Computing Platform Aimed at Speeding Scientific Work\\n\\nMarketing\\nWe create captivating digital marketing campaigns for the world’s leading Quantum Technology companies.\\nTry our\\nQuantum Market Intelligence Today\\n\\nStay Updated\\nJoin Our Newsletter\\nYou can unsubscribe anytime. For more details, review our Privacy Policy.\\n\\nFeatured News\\nAutocrypt Announces Release of Post-Quantum PKI Product for Automotive OEMs\\nIonQ Expands in EU With Slovakia’s First National Quantum Communication Network\\nQuantum Source Report Outlines Engineering Pathways to Fault-Tolerant Quantum Computing\\nAutocrypt Announces Release of Post-Quantum PKI Product for Automotive OEMs\\nIonQ Expands in EU With Slovakia’s First National Quantum Communication Network\\nQuantum Source Report Outlines Engineering Pathways to Fault-Tolerant Quantum Computing\\nNavigate\\nProjects\\nContact Us\\nLegal\\nOne of our team will be in touch to learn more about your requirements, and provide pricing and access options.\\nSubscribe to our industry leading leading newsletter for the latest in quantum news and insights.',\n",
    "# #   \"Quantum Computers News\\nTop Headlines\\nLatest Headlines\\nEarlier Headlines\\nFriday, July 25, 2025\\nThursday, July 24, 2025\\nWednesday, July 2, 2025\\nWednesday, June 25, 2025\\nThursday, June 26, 2025\\nMonday, June 30, 2025\\nTuesday, June 10, 2025\\nSaturday, June 14, 2025\\nSunday, June 8, 2025\\nThursday, May 29, 2025\\nFriday, May 23, 2025\\nFriday, May 9, 2025\\nThursday, May 8, 2025\\nTuesday, May 6, 2025\\nMonday, May 5, 2025\\nThursday, May 1, 2025\\nWednesday, April 30, 2025\\nFriday, April 25, 2025\\nFriday, April 18, 2025\\nThursday, April 17, 2025\\nTuesday, April 15, 2025\\nMonday, April 14, 2025\\nThursday, April 10, 2025\\nMonday, April 7, 2025\\nWednesday, April 2, 2025\\nThursday, March 27, 2025\\nWednesday, March 26, 2025\\nTuesday, March 25, 2025\\nFriday, March 21, 2025\\nWednesday, March 12, 2025\\nTuesday, March 11, 2025\\nWednesday, March 5, 2025\\nTuesday, March 4, 2025\\nThursday, February 27, 2025\\nWednesday, February 26, 2025\\nTuesday, February 25, 2025\\nThursday, February 20, 2025\\nWednesday, February 19, 2025\\nFriday, February 14, 2025\\nTuesday, February 11, 2025\\nMonday, February 10, 2025\\nFriday, February 7, 2025\\nWednesday, February 5, 2025\\nTuesday, February 4, 2025\\nTuesday, January 28, 2025\\nMonday, January 27, 2025\\nFriday, January 24, 2025\\nThursday, January 23, 2025\\nTuesday, January 14, 2025\\nThursday, January 9, 2025\\nMonday, January 6, 2025\\nFriday, December 20, 2024\\nThursday, December 19, 2024\\nWednesday, December 11, 2024\\nTuesday, December 10, 2024\\nWednesday, November 27, 2024\\nThursday, November 21, 2024\\nWednesday, November 13, 2024\\nTuesday, November 12, 2024\\nMonday, November 11, 2024\\nTuesday, November 5, 2024\\nThursday, October 31, 2024\\nWednesday, October 30, 2024\\nMonday, October 28, 2024\\nThursday, October 24, 2024\\nWednesday, October 23, 2024\\nTuesday, October 22, 2024\\nMonday, October 21, 2024\\nThursday, October 17, 2024\\nWednesday, October 16, 2024\\nThursday, October 3, 2024\\nWednesday, September 18, 2024\\nMonday, September 16, 2024\\nThursday, September 12, 2024\\nWednesday, September 11, 2024\\nFriday, August 30, 2024\\nThursday, August 29, 2024\\nFriday, August 23, 2024\\nThursday, August 22, 2024\\nTuesday, August 20, 2024\\nBreaking\\nTrending Topics\\nStrange & Offbeat\\nStay informed with ScienceDaily's free email newsletter, updated daily and weekly. Or view our many newsfeeds in your RSS reader:\\nKeep up to date with the latest news from ScienceDaily via social networks:\\nTell us what you think of ScienceDaily -- we welcome both positive and negative comments. Have any problems using the site? Questions?\",\n",
    "# #   'quantum computing\\n\\nWhy One VC Thinks Quantum Is a Bigger Unlock Than AGI\\n\\nA Special Diamond Is the Key to a Fully Open Source Quantum Sensor\\n\\nHow Supercomputing Will Evolve, According to Jack Dongarra\\n\\nSee How Much Faster a Quantum Computer Will Crack Encryption\\n\\nA New Quantum Algorithm Speeds Up Solving a Huge Class of Problems\\n\\nQuantum Computing Is Dead. Long Live Quantum Computing!\\n\\nThe Quantum Apocalypse Is Coming. Be Very Afraid\\n\\nMicrosoft’s New Majorana 1 Processor Could Transform Quantum Computing\\n\\nThe Incredible Power of Quantum Memory\\n\\nCryptographers Are Discovering New Rules for Quantum Encryption\\n\\nNever-Repeating Patterns of Tiles Can Safeguard Quantum Information\\n\\nApple’s iMessage Is Getting Future-Resistant Encryption\\n\\nThe Holy Grail of Quantum Computing Is Finally Here. Or Is It?\\n\\nThis Laser Can Help Verify the Source of a Diamond\\n\\nThe Quest to Use Quantum Mechanics to Pull Energy Out of Nothing\\n\\nHow Quantum Physicists ‘Flipped Time’ (and Didn’t)\\n\\nUS Technological Dominance Is Not What It Used to Be\\n\\nThe WIRED Guide to Quantum Computing\\n\\nQuantum Startups’ Stock Market Dreams Are Decohering\\n\\nQuantum Computing Has a Noise Problem\\n\\nIs Moore’s Law Really Dead?\\n\\nThree-Way Entanglement Results Hint at Better Quantum Codes\\n\\nA New Attack Easily Knocked Out a Potential Encryption Algorithm\\n\\nQuantum Advantage Showdowns Have No Clear Winners\\n\\n© 2025 Condé Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices\\nSelect international site',\n",
    "# #   \"Quantum computing\\n\\nWill quantum be bigger than AI?\\nThe highly complex technology is increasingly being tipped to transform computing.\\n\\nState-of-the-art quantum computer switched on\\nDr Chris Ballance says the new quantum computer 'uses less power than an electric kettle'.\\n\\nWhat is the weirdest thing in the universe?\\nThree scientists will be arguing the point as part of the annual Great Exhibition Road Festival.\\n\\nWill quantum computers disrupt critical infrastructure?\\nQuantum computers will be able to crack existing encryption and threaten critical infrastructure.\\n\\nPowerful quantum computers in years not decades, says Microsoft\\nIt says a new chip it has created could end up being as important a breakthrough as the semiconductor.\\n\\nQuantum computers makes things happen 'like magic'\\nResearchers from Oxford University’s physics department are among those developing quantum computing.\\n\\nOxford chosen to help lead quantum computing research\\nThe government has announced £100m of funding for the five research hubs, with one in Oxford.\\n\\n'Pivotal' quantum computing chip unveiled\\nOxford Ionics claim to have created the first quantum chip of its kind that could be mass-produced.\\n\\nHow quantum physics could 'revolutionise everything'\\nFrom unhackable communication networks to powerful computers, quantum technology promises huge advances.\\nThe BBC is in multiple languages\\nRead the BBC In your own language\\nCopyright 2025 BBC. All rights reserved. The BBC is not responsible for the content of external sites. Read about our approach to external linking.\",\n",
    "# #   \"Suggestions or feedback?\\nMIT News | Massachusetts Institute of Technology - On Campus and Around the world\\nBrowse By\\nTopics\\nDepartments\\nCenters, Labs, & Programs\\nSchools\\nBreadcrumb\\nTopic\\nQuantum computing\\nDownload RSS feed: News Articles / In the Media / Audio\\n\\nQuantum modeling for breakthroughs in materials science and sustainable energy\\nQuantum chemist and School of Science Dean’s Postdoctoral Fellow Ernest Opoku is working on computational methods to study how electrons behave.\\nNovember 19, 2025\\nRead full story →\\n\\nFrom nanoscale to global scale: Advancing MIT’s special initiatives in manufacturing, health, and climate\\nMIT.nano cleanroom complex named after Robert Noyce PhD ’53 at the 2025 Nano Summit.\\nNovember 13, 2025\\nRead full story →\\n\\nLeading quantum at an inflection point\\nThe MIT Quantum Initiative is taking shape, leveraging quantum breakthroughs to drive the future of scientific and technological progress.\\nNovember 10, 2025\\nRead full story →\\n\\nStartup provides a nontechnical gateway to coding on quantum computers\\nCo-founded by Kanav Setia and Jason Necaise ’20, qBraid lets users access the most popular quantum devices and software programs on an intuitive, cloud-based platform.\\nNovember 4, 2025\\nRead full story →\\n\\nSolar energy startup Active Surfaces wins inaugural PITCH.nano competition\\nTwelve START.nano companies competed for the grand prize of nanoBucks to be used at MIT.nano’s facilities.\\nOctober 20, 2025\\nRead full story →\\n\\nWhy some quantum materials stall while others scale\\nIn a new study, MIT researchers evaluated quantum materials’ potential for scalable commercial success — and identified promising candidates.\\nOctober 15, 2025\\nRead full story →\\n\\nLincoln Laboratory technologies win seven R&D 100 Awards for 2025\\nInventions that protect US service members, advance computing, and enhance communications are recognized among the year's most significant new products.\\nSeptember 9, 2025\\nRead full story →\\n\\nTheory-guided strategy expands the scope of measurable quantum interactions\\nAn oft-ignored effect can be used to probe an important property of semiconductors, a new study finds.\\nJuly 24, 2025\\nRead full story →\\n\\nProfessor Emeritus Daniel Kleppner, highly influential atomic physicist, dies at 92\\nThe “godfather of Bose-Einstein condensation” and MIT faculty member for 37 years led research into atomic, molecular, and optical physics that led to GPS and quantum computing.\\nJuly 15, 2025\\nRead full story →\\n\\nThe high-tech wizardry of integrated photonics\\nPhD candidate Sabrina Corsetti builds photonic devices that manipulate light to enable previously unimaginable applications, like pocket-sized 3D printers.\\nJuly 2, 2025\\nRead full story →\\n\\nNew 3D chips could make electronics faster and more energy-efficient\\nThe low-cost, scalable technology can seamlessly integrate high-speed gallium nitride transistors onto a standard silicon chip.\\nJune 18, 2025\\nRead full story →\\n\\nClosing in on superconducting semiconductors\\nPlasma Science and Fusion Center researchers created a superconducting circuit that could one day replace semiconductor components in quantum and high-performance computing systems.\\nJune 17, 2025\\nRead full story →\\n\\nMIT engineers advance toward a fault-tolerant quantum computer\\nResearchers achieved a type of coupling between artificial atoms and photons that could enable readout and processing of quantum information in a few nanoseconds.\\nApril 30, 2025\\nRead full story →\\n\\nDevice enables direct communication among multiple quantum processors\\nMIT researchers developed a photon-shuttling “interconnect” that can facilitate remote entanglement, a key step toward a practical quantum computer.\\nMarch 21, 2025\\nRead full story →\\n\\nResearchers establish new basis for quantum sensing and communication\\nNew theoretical approach for generating quantum states could lead to improved accuracy and reliability of information and decision systems.\\nMarch 13, 2025\\nRead full story →\\nPagination\\nMore about MIT News at Massachusetts Institute of Technology\\nThis website is managed by the MIT News Office, part of the Institute Office of Communications.\\nNews by Schools/College:\\nResources:\\nTools:\\nMassachusetts Institute of Technology\\n77 Massachusetts Avenue, Cambridge, MA, USA\"]}\n",
    "\n",
    "\n",
    "# state = write_initial_draft(state)\n",
    "\n",
    "# import markdown\n",
    "\n",
    "# # with open('initial_report.html', 'w') as f:\n",
    "# #     f.write(markdown.markdown(markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b540c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflect(state):\n",
    "    prompt = f\"\"\"You are an expert report evaluator.\n",
    "Your task is to evaluate the given report for:\n",
    "- structure\n",
    "- clarity\n",
    "- strength of argument, and \n",
    "- writing style\n",
    "\n",
    "Here is the report: {state['initial_draft']}\n",
    "Here is the user query: {state['query']}\n",
    "Here are the referred articles: {state['tavily_responses']}\n",
    "\n",
    "Note: Only return your feedback in a string form without any opening or closing commentary.\n",
    "\"\"\" \n",
    "    llm_gpt_5_mini = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "    response = llm_gpt_5_mini.invoke(prompt)\n",
    "    return {\"feedback\": response.content, 'query': state['query'], \"tavily_responses\": state['tavily_responses'],\n",
    "    \"initial_draft\": state['initial_draft']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = {\"query\": \"latest updates on development going on Gurgaon/Gurugram Railway Station\"}\n",
    "# state = query_tavily(state)\n",
    "\n",
    "# state = write_initial_draft(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "429127b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing reflect()\n",
    "# state = reflect(state)\n",
    "\n",
    "# print(state['feedback'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e127790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_post_reflection(state):\n",
    "    prompt = f\"\"\"You an expert essay editor.\n",
    "Your task is to edit a given essay for the given feedback.\n",
    "You are supposed to revise the essay for below points:\n",
    "- clarity\n",
    "- coherence\n",
    "- argument strength, and \n",
    "- overall flow\n",
    "\n",
    "Here is the original draft: {state['initial_draft']}\n",
    "\n",
    "The draft should be about 800-1000 words (excluding HTML tags).\n",
    "\n",
    "Strictly Follow Below Rules:  \n",
    "- Do not miss out any detail/fact/figure from the articles passed to you for reference.\n",
    "- Don't return any additional opening or closing commentary of yours (only return the title and article draft)\n",
    "- Think step by step while edit the draft. Deliver your finest work. Don't rush.\n",
    "- **Generate the report in HTML format**\n",
    "\n",
    "Here is the feedback: {state['feedback']}\n",
    "\"\"\"\n",
    "\n",
    "    llm_gpt_5_mini = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "    response = llm_gpt_5_mini.invoke(prompt)\n",
    "    return {\"final_draft\": response.content, \"feedback\": state['feedback'], 'query': state['query'], \n",
    "    \"tavily_responses\": state['tavily_responses'],\n",
    "    \"initial_draft\": state['initial_draft']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd558f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = {\"query\": \"latest updates on development going on Gurgaon/Gurugram Railway Station\"}\n",
    "# state = query_tavily(state)\n",
    "\n",
    "# state = write_initial_draft(state)\n",
    "\n",
    "# state = reflect(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdbcefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = rewrite_post_reflection(state)\n",
    "\n",
    "# with open(\"initial_report.html\", encoding = 'utf-8', mode = 'w') as f:\n",
    "#     f.write(state['initial_draft'])\n",
    "\n",
    "# with open(\"final_report.html\", encoding = 'utf-8', mode = 'w') as f:\n",
    "#     f.write(state['final_draft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5969a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"query_tavily\", query_tavily)\n",
    "builder.add_node(\"write_initial_draft\", write_initial_draft)\n",
    "builder.add_node(\"reflect\", reflect)\n",
    "builder.add_node(\"rewrite_post_reflection\", rewrite_post_reflection)\n",
    "\n",
    "builder.set_entry_point(\"query_tavily\")\n",
    "\n",
    "builder.add_edge(\"query_tavily\", \"write_initial_draft\")\n",
    "builder.add_edge(\"write_initial_draft\", \"reflect\")\n",
    "builder.add_edge(\"reflect\", \"rewrite_post_reflection\")\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54b575a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "528a3814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'latest developments towards AGI', 'tavily_responses': ['Sorry, your browser is not supported. We recommend upgrading your browser.\\n\\nFind the latest resources: white papers, reports, and videos to make the right decision for your project.\\n\\nAn ecosystem of industry leaders accelerating specialized infrastructure compute.\\n\\nDiscover why millions of developers call GitHub home & check out the arm CoPilot extension.\\n\\nThe Arm Developer Program brings together developers from across the globe and provides the perfect space to learn from leading experts, take advantage of the latest tools, and network.\\n\\nBite-sized learning videos across the full range of Arm IP.\\n\\nBe part of the world’s largest open computing ecosystem.\\nArm Account\\nNeed an Arm ID?\\nRegister here\\n\\nAchieving the Next Era of Intelligence\\nWhat Does the Future Hold for Artificial General Intelligence?\\nExplore how industry leaders are defining artificial general intelligence (AGI) and what it may take to reach it. Developed by MIT Technology Review and Arm, this deep dive examines accelerating timelines, the compute innovations shaping progress, and why today’s models still fall short of true intelligence. Designed for engineers, researchers, and technology leaders navigating the future of AI.\\nWhat\\'s Included in the Report?\\nUnderstand why experts believe artificial general intelligence could emerge by 2026 and what milestones and breakthroughs are needed to get there.\\nLearn why today’s AI models fall short of true intelligence and what capabilities like reasoning and adaptability demand from compute systems.\\nExplore how heterogeneous compute combines CPUs, GPUs, and AI accelerators to provide a practical path towards AGI workloads of the future.\\nKey Contributors\\n\\n“AI is still in the toddler phase, and like humans, the cognitive leap will take far longer.”\\n\\n“A higher level of autonomy is not the same as true intelligence.”\\n\\n“Assessing intelligence on narrow definitions of human achievement misses a spectrum of capabilities.\"\\n\\n“Reaching AGI could be about a new architecture, just like transformers led to advances in generative AI.”\\nKey Takeaways\\nStay Tuned\\nSign up for the latest AI news from Arm and to access key insights from industry-leading experts.\\n\\nThis Section is blocked in your Country', \"December 6, 2025\\n5 min read\\n\\nAre We Seeing the First Steps Toward AI Superintelligence?\\nAre We Seeing the First Steps Toward AI Superintelligence?\\nToday’s leading AI models can already write and refine their own software. The question is whether that self-improvement can ever snowball into true superintelligence\\nBy Deni Ellis Béchard edited by Eric Sullivan\\n\\nKTSDESIGN/SCIENCE PHOTO LIBRARY\\nThe Matrix, The Terminator—so much of our science fiction is built around the dangers of superintelligent artificial intelligence: a system that exceeds the best humans across nearly all cognitive domains. OpenAI CEO Sam Altman and Meta CEO Mark Zuckerberg have predicted we’ll achieve such AI in the coming years. Yet machines like those depicted as battling humanity in those movies would have to be far more advanced than ChatGPT, not to mention more capable of making Excel spreadsheets than Microsoft Copilot. So how can anyone think we’re remotely close to artificial superintelligence?\\nOne answer goes back to 1965, when statistician Irving John Good introduced the idea of an “ultraintelligent machine.” He wrote that once it became sufficiently sophisticated, a computer would rapidly improve itself. If this seems far-fetched, consider how AlphaGo Zero—an AI system developed at DeepMind in 2017 to play the ancient Chinese board game Go—was built. Using no data from human games, AlphaGo Zero played itself millions of times, achieving in days an improvement that would have taken a human a lifetime and that allowed it to defeat the previous versions of AlphaGo that had already beaten the world’s best human players. Good’s idea was that any system that was sufficiently intelligent to rewrite itself would create iterations of itself, each one smarter than the previous and even more capable of improvement, triggering an “intelligence explosion.”\\nThe question, then, is how close we are to that first system capable of autonomous self-improvement. Though the runaway systems Good described aren’t here yet, self-improving computers are—at least in narrow domains. AI is already running code on itself. OpenAI’s Codex and Anthropic’s Claude Code can work independently for an hour or more writing new code or updating existing code. Using Codex recently, I thumbed a prompt into my phone while on a walk, and it made a working website before I reached home. In the hands of skilled coders, such systems can do dramatically more, from reorganizing large code bases to sketching entirely new ways to build the software in the first place.\\nOn supporting science journalism\\nIf you're enjoying this article, consider supporting our award-winning journalism by subscribing. By purchasing a subscription you are helping to ensure the future of impactful stories about the discoveries and ideas shaping our world today.\\nSo why hasn’t a model powering ChatGPT quietly coded itself into ultraintelligence? The hitch is in the phrase above: “in the hands of skilled coders.” Despite AI’s impressive improvements, our current systems still rely on humans to set goals, design experiments and decide which changes count as genuine progress. They’re not yet capable of evolving independently in a robust way, which makes some talk about imminent superintelligence seem blown out of proportion—unless, of course, current AI systems are closer than they appear to being able to self-improve in increasingly broad slices of their abilities.\\nOne area in which they already look superhuman is how much information they can absorb and manipulate. The most advanced models are trained on far more text than any human could read in a lifetime—from poetry to history to the sciences. They can also keep track of far longer stretches of text while they work. Already, with commercially available systems such as ChatGPT and Gemini, I can upload a stack of books and have the AI synthesize and critique them in a way that would take a human weeks. That doesn’t mean the result is always correct or insightful—but it does mean that, in principle, a system like this could read its own documentation, logs, and code and propose changes at a speed and scale no engineering team could match.\\nReasoning, however, is where these systems lag—though that’s no longer true in certain focused areas. DeepMind’s AlphaDev and related systems have already found new, more efficient algorithms for tasks such as sorting, results that are now used in real-world code and that go beyond simple statistical mimicry. Other models excel at formal mathematics and graduate-level science questions that resist simple pattern-matching. We can debate the value of any particular benchmark—and researchers are doing exactly that—but there’s no question that some AI systems have become capable of discovering solutions humans had not previously found.\\nIf the systems already have these abilities, what, then, is the missing piece? One answer is artificial general intelligence (AGI), the sort of dynamic, flexible reasoning that allows humans to learn from one field and apply it to others. As I’ve previously written, we keep shifting our definitions of AGI as machines master new skills. But for the superintelligence question, what matters is not the label we attach; it’s whether a system can use its skills to reliably redesign and upgrade itself.\\nAnd this brings us back to Good’s “intelligence explosion.” If we do build systems with that kind of flexible, humanlike reasoning across many domains, what will separate it from superintelligence? Advanced models are already trained on more science and literature than any human, have far greater working memories and show extraordinary reasoning skills in limited domains. Once that missing piece of flexible reasoning is in place, and once we allow such systems to deploy those skills on their own code, data and training processes, could the leap to fully superhuman performance be shorter than we imagine?\\nNot everyone agrees. Some researchers believe we have yet to fundamentally understand intelligence and that this missing piece will take longer than expected to engineer. Others speak of AGI being achieved in a few years, leading to further advances far beyond human capacities. In 2024 Altman publicly suggested that superintelligence could arrive “in a few thousand days.”\\nIf this sounds too much like science fiction, consider that AI companies regularly run safety tests on their systems to make sure they can’t go into a runaway self-improvement loop. METR, an independent AI safety group, evaluates models according to how long they can reliably sustain a complex task before reaching failure. This past November, its tests of GPT-5.1-Codex-Max came in around two hours and 42 minutes. This is a huge leap from GPT-4’s few minutes of such performance on the same metric, but it isn’t the situation Good described.\\nAnthropic runs similar tests on its AI systems. “To be clear, we are not yet at ‘self-improving AI,’” wrote the company’s co-founder and head of policy Jack Clark in October, “but we are at the stage of ‘AI that improves bits of the next AI, with increasing autonomy.’”\\nIf AGI is achieved, and we add human-level judgment to an immense information base, vast working memory and extraordinary speed, Good’s idea of rapid self-improvement starts to look less like science fiction. The real question is whether we’ll stop at “mere human”—or risk overshooting.\\nDeni Ellis Béchard is Scientific American’s senior writer for technology. He is author of 10 books and has received a Commonwealth Writers’ Prize, a Midwest Book Award and a Nautilus Book Award for investigative journalism. He holds two master’s degrees in literature, as well as a master’s degree in biology from Harvard University. His most recent novel, We Are Dreams in the Eternal Machine, explores the ways that artificial intelligence could transform humanity. You can follow him on X, Instagram and Bluesky @denibechard\\nIt’s Time to Stand Up for Science\\nIf you enjoyed this article, I’d like to ask for your support. Scientific American has served as an advocate for science and industry for 180 years, and right now may be the most critical moment in that two-century history.\\nI’ve been a Scientific American subscriber since I was 12 years old, and it helped shape the way I look at the world. SciAm always educates and delights me, and inspires a sense of awe for our vast, beautiful universe. I hope it does that for you, too.\\nIf you subscribe to Scientific American, you help ensure that our coverage is centered on meaningful research and discovery; that we have the resources to report on the decisions that threaten labs across the U.S.; and that we support both budding and working scientists at a time when the value of science itself too often goes unrecognized.\\nIn return, you get essential news, captivating podcasts, brilliant infographics, can't-miss newsletters, must-watch videos, challenging games, and the science world's best writing and reporting. You can even gift someone a subscription.\\nThere has never been a more important time for us to stand up and show why science matters. I hope you’ll support us in that mission.\\n\\nThank you,\\nDavid M. Ewalt, Editor in Chief, Scientific American\\nSubscribe to Scientific American to learn and share the most exciting discoveries, innovations and ideas shaping our world today.\\nScientific American is part of Springer Nature, which owns or has commercial relations with thousands of scientific publications (many of them can be found at www.springernature.com/us). Scientific American maintains a strict policy of editorial independence in reporting developments in science to our readers.\\n© 2025 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC.\\nALL RIGHTS RESERVED.\", 'The Evidence for AI Consciousness, Today\\nCameron Berg\\n—\\nA growing body of evidence means it’s no longer tenable to dismiss the possibility that frontier AIs are conscious.\\n.png)\\nAI Alignment Cannot Be Top-Down\\nAudrey Tang\\n—\\nCommunity Notes offers a better model — where citizens, not corporations, decide what “aligned” means.\\nAI alignment, attentiveness, Community Notes, Taiwan, Audrey Tang, model specification, deliberative governance, epistemic security, portability and interoperability, market design, Polis, reinforcement learning from community feedback, social media moderation, civic technology\\n\\nAGI\\'s Last Bottlenecks\\nAdam Khoja\\n—\\nA new framework suggests we’re already halfway to AGI. The rest of the way will mostly require business-as-usual research and engineering.\\nAGI, artificial general intelligence, AGI definition, GPT-5, GPT-4, visual reasoning, world modeling, continual learning, long-term memory, hallucinations, SimpleQA, SPACE benchmark, IntPhys 2, ARC-AGI, working memory\\n\\nAI Will Be Your Personal Political Proxy\\nBruce Schneier\\n—\\nBy learning our views and engaging on our behalf, AI could make government more representative and responsive — but not if we allow it to erode our democratic instincts.\\nAI political proxy, direct democracy, generative social choice, ballot initiatives, voter participation, democratic representation, AI governance, Rewiring Democracy, Bruce Schneier, Nathan E. Sanders, policy automation, civic engagement, rights of nature, disenfranchised voters, algorithmic policymaking\\n\\nIs China Serious About AI Safety?\\nKarson Elmgren\\n—\\nChina’s new AI safety body brings together leading experts — but faces obstacles to turning ambition into influence.\\nChina AI Safety and Development Association, CnAISDA, China AI safety, World AI Conference, Shanghai AI Lab, Frontier AI risk, AI governance, international cooperation, Tsinghua University, CAICT, BAAI, Global AI Governance Action Plan, AI Seoul Summit commitments, Concordia AI, Entity List\\n\\nAI Deterrence Is Our Best Option\\nDan Hendrycks\\n—\\nA response to critiques of Mutually Assured AI Malfunction (MAIM).\\nAI deterrence, Mutually Assured AI Malfunction, MAIM, Superintelligence Strategy, ASI, intelligence recursion, nuclear MAD comparison, escalation ladders, verification and transparency, redlines, national security, sabotage of AI projects, deterrence framework, Dan Hendrycks, Adam Khoja\\n\\nSummary of “If Anyone Builds It, Everyone Dies”\\nLaura Hiscott\\n—\\nAn overview of the core arguments in Yudkowsky and Soares’s new book.\\nIf Anyone Builds It Everyone Dies, Eliezer Yudkowsky, Nate Soares, MIRI, AI safety, AI alignment, artificial general intelligence, artificial superintelligence, AI existential risk, Anthropic deceptive alignment, OpenAI o1, Truth_Terminal, AI moratorium, book summary, Laura Hiscott\\n\\nAI Agents Are Eroding the Foundations of Cybersecurity\\nRosario Mastrogiacomo\\n—\\nIn this age of intelligent threats, cybersecurity professionals stand as the last line of defense. Their decisions shape how humanity contends with autonomous systems.\\nAI agents, AI identities, cybersecurity, identity governance, zero trust, least privilege, rogue AI, autonomous systems, enterprise security, trust networks, authentication and authorization, RAISE framework, identity security, circuit breakers\\n\\nPrecaution Shouldn\\'t Keep Open-Source AI Behind the Frontier\\nBen Brooks\\n—\\nInvoking speculative risks to keep our most capable models behind paywalls could create a new form of digital feudalism.\\nopen-source AI, frontier models, precautionary policy, digital feudalism, OpenAI, Meta, Llama, GPT-OSS, regulation, open development, AI risk, legislation, policy debate, Berkman Klein Center\\n\\nThe Hidden AI Frontier\\nOscar Delaney\\n—\\nMany cutting-edge AI systems are confined to private labs. This hidden frontier represents America’s greatest technological advantage — and a serious, overlooked vulnerability.\\nhidden frontier AI, internal AI models, AI security, model theft, sabotage, government oversight, transparency, self-improving AI, AI R&D automation, policy recommendations, national security, RAND security levels, frontier models, AI governance, competitive advantage\\n\\nUncontained AGI Would Replace Humanity\\nAnthony Aguirre\\n—\\nThe moment AGI is widely released — whether by design or by breach — any guardrails would be as good as gone.\\nAGI, artificial general intelligence, open-source AI, guardrails, uncontrolled release, existential risk, humanity replacement, security threat, proliferation, autonomous systems, alignment, self-improving intelligence, policy, global race, tech companies\\n\\nSuperintelligence Deterrence Has an Observability Problem\\nJason Ross Arnold\\n—\\nMutual Assured AI Malfunction (MAIM) hinges on nations observing one another\\'s progress toward superintelligence — but reliable observation is harder than MAIM\\'s authors acknowledge.\\nMAIM, superintelligence deterrence, Mutual AI Malfunction, observability problem, US-China AI arms race, compute chips data centers, strategic sabotage, false positives, false negatives, AI monitoring, nuclear MAD analogue, superintelligence strategy, distributed R&D, espionage escalation, peace and security\\n\\nOpen Protocols Can Prevent AI Monopolies\\nIsobel Moure\\n—\\nWith model performance converging, user data is the new advantage — and Big Tech is sealing it off.\\nopen protocols, AI monopolies, Anthropic MCP, context data lock-in, big tech, APIs, interoperability, data portability, AI market competition, user context, model commoditization, policy regulation, open banking analogy, enshittification\\n\\nIn the Race for AI Supremacy, Can Countries Stay Neutral?\\nAnton Leicht\\n—\\nThe global AI order is still in flux. But when the US and China figure out their path, they may leave little room for others to define their own.\\nAI race, US-China competition, middle powers, export controls, AI strategy, militarization, economic dominance, compute supply, frontier models, securitization, AI policy, grand strategy, geopolitics, technology diffusion, national security\\n\\nHow AI Can Degrade Human Performance in High-Stakes Settings\\nDane A. Morey\\n—\\nAcross disciplines, bad AI predictions have a surprising tendency to make human experts perform worse.\\nAI, human performance, safety-critical settings, Joint Activity Testing, human-AI collaboration, AI predictions, aviation safety, healthcare alarms, nuclear power plant control, algorithmic risk, AI oversight, cognitive systems engineering, safety frameworks, nurses study, resilient performance\\n.jpg)\\nHow the EU\\'s Code of Practice Advances AI Safety\\nHenry Papadatos\\n—\\nThe Code provides a powerful incentive to push frontier developers toward measurably safer practices.\\nEU Code of Practice, AI Act, AI safety, frontier AI models, risk management, systemic risks, 10^25 FLOPs threshold, external evaluation, transparency requirements, regulatory compliance, general-purpose models, European Union AI regulation, safety frameworks, risk modeling, policy enforcement\\n\\nHow US Export Controls Have (and Haven\\'t) Curbed Chinese AI\\nChris Miller\\n—\\nSix years of export restrictions have given the U.S. a commanding lead in key dimensions of the AI competition — but it’s uncertain if the impact of these controls will persist.\\nchip, chips, china, chip export controls, China semiconductors, hardware, AI hardware policy, US technology restrictions, SMIC, Huawei Ascend, Nvidia H20, AI infrastructure, high-end lithography tools, EUV ban, domestic chipmaking, AI model development, technology trade, computing hardware, US-China relations\\n\\nNuclear Non-Proliferation Is the Wrong Framework for AI Governance\\nMichael C. Horowitz\\n—\\nPlacing AI in a nuclear framework inflates expectations and distracts from practical, sector-specific governance.\\n\\nA Patchwork of State AI Regulation Is Bad. A Moratorium Is Worse.\\nKristin O’Donoghue\\n—\\nCongress is weighing a measure that would nullify thousands of state AI rules and bar new ones — upending federalism and halting the experiments that drive smarter policy.\\nai regulation, state laws, federalism, congress, policy innovation, legislative measures, state vs federal, ai governance, legal framework, regulation moratorium, technology policy, experimental policy, state experimentation, federal oversight, ai policy development\\n\\nCan Copyright Survive AI?\\nLaura González Salmerón\\n—\\nDesigned to protect human creativity, copyright law is under pressure from generative AI. Some experts question whether it has a future.\\ncopyright, generative ai, ai, creativity, intellectual property, law, legal challenges, technology, digital rights, innovation, future of copyright, authorship, content creation, legal reform, copyright law, ai-generated content\\n\\nAvoiding an AI Arms Race with Assurance Technologies\\nNora Ammann\\n—\\nA global race to build powerful AI is not inevitable. Here’s how technical solutions can help foster cooperation.\\nai arms race, assurance technologies, ai cooperation, global ai development, technical solutions, ai safety, international collaboration, ethical ai, ai policy, ai governance, technology diplomacy, nuclear\\n\\nWe\\'ll Be Arguing for Years Whether Large Language Models Can Make New Scientific Discoveries\\nEdward Parker\\n—\\nai, artificial intelligence, large language models, scientific discovery, digital intelligence, expert consensus, technology, innovation, society impact, machine learning, research, future of science, debate, ai capabilities, advancements in ai\\n\\nThe Case for AI Liability\\nGabriel Weil\\n—\\nAbandoning liability mechanisms risks creating a dangerous regulatory vacuum.\\nai liability, regulatory vacuum, liability mechanisms, ai regulation, legal frameworks, technology accountability, risk management, artificial intelligence, governance, policy, ethical ai, tech industry, innovation, legal responsibility\\n\\nWhat if Organizations Ran Themselves?\\nGayan Benedict\\n—\\nAutonomous AI-enabled organizations are increasingly plausible. They would fundamentally break the way we regulate the economy.\\nautonomous organizations, ai-enabled organizations, self-managing organizations, economic regulation, artificial intelligence, future of work, organizational structure, automation, technology in business, decentralized management, ai in economics, innovation, business transformation, nuclear\\n\\nHow AI Can Prevent Blackouts\\nDavid \\'davidad\\' Dalrymple\\n—\\nFor safety-critical domains like energy grids, \"probably safe\" isn\\'t good enough. To fulfill the potential of AI in these areas, we need to develop more robust, mathematical guarantees of safety.\\nai, energy grids, blackout prevention, safety-critical domains, mathematical guarantees, robust ai, infrastructure safety, power systems, risk management, smart grids, technology in energy, ai safety, nuclear\\n\\nWe\\'re Not Ready for AI Liability\\nKevin Frazier\\n—\\nIn the absence of federal legislation, the burden of managing AI risks has fallen to judges and state legislators — actors lacking the tools needed to ensure consistency, enforceability, or fairness.\\nai liability, federal legislation, ai risks, judges, state legislators, legal challenges, consistency, enforceability, fairness, regulation, technology policy, artificial intelligence, legal framework, risk management, governance, state laws, judicial responsibility\\n\\nA Glimpse into the Future of AI Companions\\nVanessa Bates Ramirez\\n—\\nAI is increasingly being used for emotional support — but research from OpenAI and MIT raises concerns that it may leave some users feeling even worse.\\nai companions, emotional support, openai, mit, mental health, technology, future of ai, ethical concerns, user experience, psychological impact, artificial intelligence, digital companionship, ai ethics, emotional well-being, human-ai interaction\\n\\nHow AI Is Eroding the Norms of War\\nDavid Kirichenko\\n—\\nAn unchecked autonomous arms race is eroding rules that distinguish civilians from combatants.\\nai, autonomous weapons, arms race, warfare norms, civilian protection, military ethics, combatants, war technology, international law, defense policy, unmanned systems, ethical concerns, artificial intelligence, conflict dynamics, security challenges, nuclear\\n\\nToday\\'s AIs Aren\\'t Paperclip Maximizers. That Doesn\\'t Mean They\\'re Not Risky\\nPeter N. Salib\\n—\\nClassic arguments about AI risk imagined AIs pursuing arbitrary and hard-to-comprehend goals. Large Language Models aren\\'t like that, but they pose risks of their own.\\nai risk, paperclip maximizer, large language models, ai goals, ai safety, ai ethics, ai threats, ai behavior, ai development, technology risks, artificial intelligence, machine learning, ai impacts, existential risk, ai governance\\n\\nCan “Location Verification” Stop AI Chip Smuggling?\\nScott J Mulligan\\n—\\nUS lawmakers propose a new system to check where chips end up.\\nai chip smuggling, location verification, us lawmakers, chip tracking, technology regulation, semiconductor industry, export control, national security, supply chain monitoring, tech policy, chip distribution, international trade, compliance technology\\n\\nThe Misguided Quest for Mechanistic AI Interpretability\\nDan Hendrycks\\n—\\nDespite years of effort, mechanistic interpretability has failed to provide insight into AI behavior — the result of a flawed foundational assumption.\\nmechanistic interpretability, ai behavior, ai transparency, ai ethics, machine learning, flawed assumptions, ai research, ai analysis, ai limitations, ai insights\\n\\nWe’re Arguing About AI Safety Wrong\\nHelen Toner\\n—\\nDynamism vs. stasis is a clearer lens for criticizing controversial AI safety prescriptions.\\nai safety, ai ethics, dynamism, stasis, artificial intelligence, technology criticism, safety prescriptions, ai development, risk assessment, innovation vs regulation, tech debate, ai policy, future of ai\\n.avif)\\nCan the US Prevent AGI from Being Stolen?\\nPhilip Tschirhart\\n—\\nSecuring AI weights from foreign adversaries would require a level of security never seen before.\\nartificial general intelligence, agi, ai security, cybersecurity, national security, us defense, intellectual property, technology theft, foreign adversaries, ai research, ai ethics, ai governance, data protection, tech policy, ai innovation, nuclear\\n.avif)\\nAI Companies Want to Give You a New Job. Your Team? A Million AIs.\\nVanessa Bates Ramirez\\n—\\nAI Frontiers spoke with leading researchers and a CEO building AI agents to explore how AI will reshape work—and whether the jobs of the future are ones we’ll actually want.\\nai, future of work, automation, ai companies, job transformation, ai researchers, ai agents, workplace innovation, employment trends, ai impact, digital workforce, technology and jobs, ai in business, ai ceo, ai frontier\\n\\nAmerica First Meets Safety First\\nMiles Brundage\\n—\\nPresident Trump vowed to be a peacemaker. Striking an “AI deal” with China could define global security and his legacy.\\namerica first, safety first, president trump, peacemaker, china, ai safety, global security, international relations, diplomacy, legacy, artificial intelligence, us-china relations, geopolitics, technology policy, nuclear\\n.avif)\\nAIs Are Disseminating Expert-Level Virology Skills\\nDan Hendrycks\\n—\\nNew research shows frontier models outperform human scientists in troubleshooting virology procedures—lowering barriers to the development of biological weapons.\\nai, virology, biological weapons, research, frontier models, bio lab, artificial intelligence, expert-level skills, human scientists, biosecurity, technology, laboratory tasks, scientific innovation\\n.avif)\\nSmokescreen: How Bad Evidence Is Used to Prevent AI Safety\\nLaura Hiscott\\n—\\nCorporate capture of AI research—echoing the days of Big Tobacco—thwarts sensible policymaking.\\nai safety, bad evidence, ai policy, flawed benchmarks, corporate influence, transparency, accountability, safety data, research environment, structural reforms, trustworthy data, ai research, evidence-based policy\\n\\nWe Need a New Kind of Insurance for AI Job Loss\\nKevin Frazier\\n—\\nAI is poised to leave a lot of us unemployed. We need to rethink social welfare.\\nai job loss, social insurance, ai displacement, future of work, automation, labor market, economic policy, job displacement, workforce adaptation, technology impact, unemployment, us economy, ai policy, social safety net, employment insurance\\n\\nExporting H20 Chips to China Undermines America’s AI Edge\\nJason Hausenloy\\n—\\nContinued sales of advanced AI chips allow China to deploy AI at massive scale.\\nai, china, h20 chips, advanced gpus, technology export, global ai race, us-china relations, semiconductor industry, technology policy, national security, america\\'s ai edge, trade restrictions, tech competition, geopolitical tension\\n.avif)\\nHow Applying Abundance Thinking to AI Can Help Us Flourish\\nKevin Frazier\\n—\\nRealizing AI’s full potential requires designing for opportunity—not just guarding against risk.\\nabundance thinking, ai, artificial intelligence, potential, hope, fear, positive mindset, growth, technology, innovation, future, opportunities, human flourishing, optimistic outlook\\n\\nWhy Racing to Artificial Superintelligence Would Undermine America’s National Security\\nCorin Katzke\\n—\\nRather than rushing toward catastrophe, the US and China should recognize their shared interest in avoiding an ASI race.\\nartificial superintelligence, ASI, national security, US-China relations, technology race, AI ethics, global cooperation, AI policy, security risks, international relations, technological competition, AI development, strategic interests, AI governance, catastrophic risk, nuclear\\n\\nAI Risk Management Can Learn a Lot From Other Industries\\nMalcolm Murray\\n—\\nAI risk may have unique elements, but there is still a lot to be learned from cybersecurity, enterprise, financial, and environmental risk management.\\nai, risk management, cybersecurity, enterprise risk, financial risk, environmental risk, industry comparison, best practices, risk mitigation, technology, innovation, safety protocols, governance, compliance, regulations, nuclear\\n\\nCan We Stop Bad Actors From Manipulating AI?\\nAndy Zou\\n—\\nAI is naturally prone to being tricked into behaving badly, but researchers are working hard to patch that weakness.\\nai security, adversarial attacks, machine learning, ai ethics, cybersecurity, ai manipulation, bad actors, ai vulnerabilities, defense mechanisms, ai research, algorithmic bias, ethical ai, ai safety, trust in ai\\n\\nThe Challenges of Governing AI Agents\\nNoam Kolt\\n—\\nAutonomous systems are being rapidly deployed, but governance efforts are still in their infancy.\\nai governance, autonomous systems, ai deployment, regulatory challenges, ethical ai, ai policy, technology governance, ai ethics, ai regulation, autonomous agents, ai oversight, responsible ai, ai safety, emerging technologies, ai accountability\\n\\nWelcome to AI Frontiers\\nThe AI Frontiers Editorial Board\\n—\\nai, artificial intelligence, machine learning, technology, innovation, future, data science, ai development, ai research, ai trends, automation, deep learning\\nAGI\\'s Last Bottlenecks\\nA new framework suggests we’re already halfway to AGI. The rest of the way will mostly require business-as-usual research and engineering.\\n\\nAdam Khoja is a co-author of the recent study, “A Definition of AGI.” The opinions expressed in this article are his own and do not necessarily represent those of the study’s other authors.\\nLaura Hiscott is a core contributor at AI Frontiersand collaborated on the development and writing of this article.\\n\\u200dDan Hendrycks, lead author of “A Definition of AGI,” provided substantial input throughout this article’s drafting.\\n_____\\nIn a recent interview on the “Dwarkesh Podcast,” OpenAI co-founder Andrej Karpathy claimed that artificial general intelligence (AGI) is around a decade away, expressing doubt about “over-predictions in the industry.” Coming amid growing discussion of an “AI bubble,” Karpathy’s comment throws cold water on some of the more bullish predictions from leading tech figures. Yet those figures don’t seem to be reconsidering their positions. Following Anthropic CEO Dario Amodei’s prediction last year that we might have “a country of geniuses in a datacenter” as early as 2026, Anthropic co-founder Jack Clark said this September that AI will be smarter than a Nobel Prize winner across many disciplines by the end of 2026 or 2027.\\nA testable AGI definition is needed for apples-to-apples comparisons. There may be as many estimates of when AGI will arrive as there are people working in the field. And to complicate matters further, there is disagreement on what AGI even is. This imprecision hampers attempts to compare forecasts. To provide clarity to the debate, we, alongside thirty-one co-authors, recently released a paper that develops a detailed definition of AGI, allowing us to quantify how well models “can match or exceed the cognitive versatility and proficiency of a well-educated human adult.” We don’t claim our definition represents exactly what Karpathy or Amodei imagine when they discuss future AI systems, but a precise specification of AGI does provide the starting point for an apples-to-apples debate.\\n\\nOur framework scores ten broad abilities and finds GPT-5 roughly halfway to AGI. Inspired by the Cattell-Horn-Carroll (CHC) theory of human intelligence, our definition formulates AGI as a system that possesses 10 broad abilities found in humans, from knowledge and reasoning to memory and writing. Just as with the study of human intelligence, we have a battery of diverse tasks that can rigorously assess AI models’ performance in each of these areas. We have tested GPT-4 and GPT-5 with targeted benchmarks in each of the 10 capabilities, weighting them equally to calculate an overall “AGI score.” Based on our definition, GPT-4 achieved a score of 27%, while GPT-5 reached 57% — with GPT-5’s main improvements being image support, audio support, a much larger context window, and mathematical skills.\\nIn order to predict when AGI — as defined by this framework — will arrive, we can systematically analyze each of the areas where the models fall short of well-educated humans, quantify how quickly systems are progressing, and estimate how difficult each barrier will be to resolve. As we will discuss, the meatiest remaining challenges appear to be visual processing and continual learning, but they ultimately appear tractable.\\nMissing Capabilities and the Path to Solving Them\\nTo judge proximity to AGI, focus on where models still fall short, not excel. For the purposes of this analysis, we won’t look at areas where current models are already performing at or above human baselines. We are now well used to LLMs that broadly match or exceed most well-educated humans at reading, writing, and math. Having been trained on the internet, these models also have far superior reserves of knowledge than any human. To understand how close current models are to AGI, we must focus on where they lose points in the definitional framework, and how tractable those areas are.\\nAI advances can generally be placed in one of three categories: (1) “business-as-usual” research and engineering that is incremental; (2) “standard breakthroughs” at a similar scale to OpenAI’s advancement that delivered the first reasoning models in 2024; finally, (3) “paradigm shifts” that reshape the field, at the scale of pretrained Transformers. We will now look at the areas of our definition where GPT-4 and GPT-5 lose many points — reasoning, visual processing, auditory processing, speed, working memory, memory retrieval, and long-term memory storage — and assess the scale of advance required to reach our definition of AGI.\\nVisual Processing\\nOverview. A reasonable (but imperfect) way to describe the previous generation of vision capabilities is by analogy to a human who is shown an image for a fraction of a second before they must answer questions about it: enough time to recognize natural objects and describe scenes, but not enough to count objects or perform mental rotations. Current models are more capable, but still struggle on visual reasoning and world modeling.\\n\\nVisual reasoning. While models can readily understand simple natural images, unnatural images such as schematics and screenshots are more challenging. That said, state-of-the-art models are rapidly improving their understanding of unnatural images and are becoming more capable visual reasoners. For example, on a subset of the SPACE benchmark developed by Apple, GPT-4o (May 2024) scored only 43.8%, while internal tests we’ve done at the Center for AI Safety show GPT-5 (August 2025) scoring 70.8%, while humans get 88.9% on average. Therefore, we might expect that business-as-usual AI development will continue to drive rapid progress on visual reasoning.\\n\\nWorld modeling. Another visual processing task that models struggle with is world modeling, or intuitively understanding how the physical world behaves. Researchers such as Yann LeCun have argued that more fundamental advances may be needed to achieve this capability. A recent benchmark from Meta called IntPhys 2 tests world modeling by presenting AIs with videos and asking them how physically plausible the scenarios are. It shows that the best current models perform only slightly better than chance. However, upstream capabilities progress is a tide that lifts many boats, so it would not surprise us to see significant improvements on this benchmark with business-as-usual engineering.\\nOn-the-Spot Reasoning\\nProgress in on-the-spot reasoning in the past two years has been substantial. While GPT-4 struggled with simple logical puzzles, reasoning models such as GPT-5 now approach the fluidity and precision of humans, especially in the text modality. By thinking about complex problems for hours on end, the best language models now score well enough on various olympiads, including the International Olympiad in Informatics (IOI) and International Math Olympiad (IMO), to earn gold medals.\\n\\nModels still struggle with visual induction. Despite the ongoing improvements in on-the-spot reasoning generally, models still lose points on tasks that demand visual induction. For example, they perform worse than most humans in a visual reasoning IQ test called Raven’s Progressive Matrices. Yet, when presented with text descriptions of the same problems, top models score between 15 to 40 points better than when given the raw question images, exceeding most humans. This suggests the modality is what is making the difference, rather than a deficiency in the model’s logical reasoning itself.\\n\\nThe remaining bottleneck is likely perception, not reasoning. Issues with visual inductive reasoning are the main barrier to perfect reasoning scores. In a benchmark measuring this called ARC-AGI, models have made impressive strides but remain below human-level. However, this may be due to a difficulty in perceiving visual data. An engineer demonstrated that logically identical but enlarged ARC-AGI puzzles produced a large drop in models’ performance. Humans, with their extremely fluent visual processing abilities, would hardly notice the difference, while models see it as a significantly longer and more difficult problem. This suggests that remaining deficiencies in visual reasoning may be more a failure of perception, rather than a failure of the underlying capacity to reason.\\nImprovements in multimodal perception are therefore plausibly much of what is required for a human-level on-the-spot reasoning score, and these improvements could be delivered by business-as-usual engineering.\\nAuditory Processing\\nAudio capabilities appear tractable if given greater prioritization. Historically, audio capabilities tend to be easier for models to learn than visual capabilities. Current deficiencies in audio processing may simply be because this domain is not the highest priority in large AI companies, not because researchers don’t know how to make progress here. This is borne out by Sesame AI, a startup making voice companions, whose voice models from this past winter still far outperform the state of the art from frontier AI corporations. Putting in more effort to train models using known techniques on better auditory data (e.g., clearly labeled emotive interjections and accents in audio data) and reducing latency may therefore be sufficient to fill much of the gap. We therefore expect that business-as-usual engineering will saturate this domain.\\nSpeed\\nSpeed is superhuman in text and math, but lags where perception or tool use is required. When it comes to speed — a component of intelligence that considers how quickly a model can complete tasks — the scores vary depending on modality. GPT-5 is much faster than humans at reading, writing, and math, but slower at certain auditory, visual, and computer use tasks. In some cases, GPT-5 also seems to use reasoning mode to complete fairly simple tasks that should not require much reasoning, meaning that they take an unnecessarily long, convoluted approach that slows them down. Nonetheless, at fixed performance levels, costs and speed have improved dramatically year on year. Improving speed across many areas is a business-as-usual activity in frontier AI corporations, where known methods are yielding steady progress.\\nWorking Memory\\nAnother area where both models dropped points was in their working memory — the ability to maintain and manipulate information in active attention. There are multiple facets to this capability, with information being presented in textual, auditory, and visual modalities. When working with text, current models already demonstrate a working memory comparable with humans, if not far superior. Meanwhile, tasks that assess working memory in the visual and auditory modalities are what models struggle with. For example, one task that falls within visual working memory is spatial navigation memory. On a benchmark called MindCube, which measures this, GPT-4o scores 38.8%, far below human level. GPT-5 shows considerable improvement, achieving 59.7%, though this is still below average human scores. Improving auditory working memory is likely to be even more tractable than visual working memory. Since models already have a human-level working memory for text, it seems likely that business-as-usual engineering will bring the visual and auditory modalities along too.\\n\\nLong-Term Memory Retrieval (Hallucinations)\\nLong-term memory retrieval is another core component of general intelligence, and models already have an impressive ability to fluently access information in their vast stores of general knowledge. Where GPT-4 and GPT-5 lose points, however, is in their tendency to hallucinate — to utter inaccurate information without hesitation. One of the current best available measures for hallucination is SimpleQA, a benchmark created by OpenAI that gives models highly specific questions to answer without using the internet. To score well, a model must not just be good at retrieving information, but also be able to recognize when it is uncertain — in many cases, it should refuse to answer, rather than making something up. Both GPT-4 and GPT-5 perform poorly in this area, with the latter hallucinating in response to more than 30% of questions. However, Anthropic’s Claude models hallucinate far less often, approaching but not yet matching human-level confabulation rates. This suggests the problem is tractable with business-as-usual engineering.\\n\\nLong-Term Memory Storage (Continual Learning)\\nThe only broad domain in which GPT-4 and GPT-5 both score zero is long-term memory storage, or continual learning — the capacity to keep learning from new experiences and adapting behavior over the long term. Current models are “frozen” after training, preventing them from meaningfully learning anything new in deployment.\\nAlthough models can do a “capability contortion,” leaning into their strong working memories over long context windows to give a false impression of long-term memory, this is not practical over weeks or months. They still have a kind of “amnesia,” resetting with every new session. To mimic a human’s capacity for continual learning, a dedicated long-term memory solution is essential, perhaps in the form of durable weight updates.\\nOf all the gaps between today’s models and AGI, this is the most uncertain in terms of timeline and resolution. Every missing capability we have discussed so far can probably be achieved by business-as-usual engineering, but for continual long-term memory storage, we need a breakthrough. Nonetheless, the problem is not completely opaque, and probably won’t require a paradigm shift.\\nThe problem is now receiving substantial attention and resources from frontier AI corporations. In August, Demis Hassabis highlighted memory as a key missing capability, while Sam Altman, talking about GPT-6, hinted, “People want memory. People want product features that require us to be able to understand them.” Dario Amodei represented the sentiment of much of the industry when he said during an interview in August: “Models learn within the context… Maybe we\\'ll train the model in such a way that it is specialized for learning over the context. You could, even during the context, update the model\\'s weights… So, there are lots of ideas that are very close to the ideas we have now that could perhaps do [continual learning].”\\nDespite being arguably the most nebulous remaining obstacle to AGI, we may only need an o1-preview moment for continual learning and long-term memory storage, that is, a standard breakthrough away.\\nConclusion\\nDrawing together all the capabilities detailed in our framework, we can think of general intelligence as a cognitive “engine” that transforms inputs into outputs. We think our definition is useful because it provides a framework for assessing AIs against the breadth of human cognitive capabilities, so we can pinpoint which specific skills are still missing; the capability of the full system is arguably only as strong as its weakest link.\\n\\nConsidering the gaps that we have highlighted, what will it take to get to AGI? According to our analysis, all that may be needed is a breakthrough in continual learning, as well as regular research and engineering for handling visual reasoning, world modeling, hallucinations, and spatial navigation memory.\\nLooking at how research is advancing in each of these areas, when might we expect a company to publicly release a model with an AGI Score above 95% according to our definition? One of the authors, Adam, estimates a 50% chance of reaching this threshold by the end of 2028, and an 80% chance by the end of 2030.\\nUltimately, our framework allows us to replace vague speculation with a quantitative diagnostic. Given the industry’s focused efforts, it seems highly plausible that researchers will fill in all the puzzle pieces of our definition of AGI in the next few years, much sooner than the decade-long timelines suggested by some in the field. We are a standard breakthrough and business-as-usual research away from AGI.\\n\\nAdam Khoja\\nAdam Khoja does technical and policy research at the Center for AI Safety. He studied math and computer science at UC Berkeley.\\n\\nLaura Hiscott\\nLaura is a staff writer for AI Frontiers. She has worked in science communication for over six years, both in press offices and at magazines. She studied physics at Imperial College London and trained in science communication at the European Southern Observatory.\\n\\nThe Evidence for AI Consciousness, Today\\nA growing body of evidence means it’s no longer tenable to dismiss the possibility that frontier AIs are conscious.\\n.png)\\nAI Alignment Cannot Be Top-Down\\nCommunity Notes offers a better model — where citizens, not corporations, decide what “aligned” means.\\nSubscribe to AI Frontiers\\nSubscribe to AI Frontiers\\n.svg)\\nAI Frontiers is a platform for expert dialogue and debate on the impacts of artificial intelligence.\\nThe views expressed in our articles reflect the perspectives of individual authors, not necessarily those of the editors or the publication as a whole. Our editorial team values intellectual variety and believes that AI is a complex topic demanding a range of viewpoints, carefully considered.\\n\\nSubscribe to AI Frontiers\\n\\nDiscover more from AI Frontiers\\nStay informed on the future of AI alongside\\n other subscribers.\\nI\\'ve already subscribed', 'When Will AGI/Singularity Happen? 8,590 Predictions Analyzed\\n\\nWe analyzed 8,590 scientists’, leading entrepreneurs’, and the community’s predictions for quick answers on Artificial General Intelligence (AGI) / singularity timeline:\\nExplore key predictions on AGI from experts like Sam Altman and Demis Hassabis, insights from major AI surveys on AGI timelines, and arguments for and against the feasibility of AGI:\\nArtificial General Intelligence timeline\\nThis timeline outlines the anticipated year of the singularity, based on insights gathered from 15 surveys, including responses from 8,590 AI researchers, scientists, and participants in prediction markets:\\nAs you can see above, survey respondents are increasingly expecting the singularity to occur earlier than previously expected.\\nHere is how we created this graph:\\nBelow you can see the studies and predictions that make up this timeline, or skip to understanding the singularity.\\nResults of major surveys of AI researchers\\nWe examined the results of 10 surveys involving over 5,288 AI researchers and experts, where they estimated when AGI/singularity might occur.\\nWhile predictions vary, most surveys indicate a 50% probability of achieving AGI between 2040 and 2061, with some estimating that superintelligence could follow within a few decades.\\nAAAI 2025 Presidential Panel on the Future of AI Research\\n475 respondents, mainly from academia (67%) and North America (53%), were asked about progress in AI. Though the survey didn’t ask for a timeline for AGI, 76% of respondents shared that scaling up current AI approaches would be unlikely to lead to AGI.2\\n2023 Expert Survey on Progress in AI\\nIn October, AI Impacts surveyed 2,778 AI researchers on when AGI might be achieved. This survey included nearly identical questions to the 2022 survey. Based on the results, the high-level machine intelligence is estimated to occur by 2040.3\\n2022 Expert Survey on Progress in AI\\nThe survey was conducted with 738 experts who published at the 2021 NIPS and ICML conferences. AI experts estimate that there’s a 50% chance that high-level machine intelligence will occur by 2059.4\\nExperts also predicted that hardware cost, algorithmic progress, and work on training sets would be the biggest factors in AI progress.\\nForecasting AI progress survey in 2019\\nBaobao Zhang surveyed 296 AI experts, asking them to predict when machines would surpass the median human worker in performing over 90% of economically relevant tasks. Half of the respondents estimated this would happen before 2060.5\\nAI experts’ survey on AGI timing in 2019\\nThe predictions of 32 AI experts on AGI timing6 are:\\nSurvey on AI’s potential impact on labor displacement in 2018\\nRoss Gruetzemacher surveyed 165 AI experts to assess the potential impact of AI on labor displacement. The experts were asked to estimate when AI systems would be capable of performing 99% of tasks for which humans are currently paid, at a level equal to or exceeding that of an average human.\\nHalf of the respondents predicted this milestone would be reached before 2068, while 75% anticipated it would occur within the next 100 years.7\\nAI experts in the 2015 NIPS and ICML conferences survey in 2017\\nIn May 2017, 352 AI experts who published at the 2015 NIPS and ICML conferences were surveyed.8\\nBased on survey results, experts estimate that there’s a 50% chance that AGI will occur by 2060. That said, there’s a significant difference of opinion based on geography:\\nSome significant job functions expected to be automated by 2030 include call center reps, truck driving, and retail sales.\\nFuture Progress in Artificial Intelligence survey in 2012/2013\\nVincent C. Muller, the president of the European Association for Cognitive Systems, and Nick Bostrom from the University of Oxford, who published over 200 articles on superintelligence and artificial general intelligence (AGI), conducted a survey of AI researchers. 550 participants answered the question: When is AGI likely to happen?9\\nAccording to the results:\\n2009 survey with AI experts participating in the AGI-09 conference\\nBased on the results of the survey with 21 AI experts participating in the AGI-09 conference, it is believed that AGI will occur around 2050, and plausibly sooner.10 You can see below their estimates regarding specific AI achievements: passing the Turing test, passing third grade, accomplishing Nobel-worthy scientific breakthroughs, and achieving superhuman intelligence.\\nFigure 1: Results from the survey distributed to attendees of the Artificial General Intelligence 2009 (AGI-09) conference.\\nOther comments and developments about AGI\\nOpenAI expands its robotics ambitions\\nOpenAI is increasing its focus on robotics as part of its goal to advance artificial general intelligence. The company is hiring specialists in humanoid robot systems and forming a team to design algorithms that help robots learn and act independently in the physical world.\\nThis marks a shift from OpenAI’s earlier focus on language and image models. The company now aims to connect advanced reasoning with physical interaction, suggesting it views robotics as an essential step toward testing and achieving AGI.\\nContext and implications\\nAfter winding down its first robotics team around 2020, OpenAI is returning to active development in the field. Recent hiring and potential partnerships point to a renewed effort to build robots capable of real-world learning and manipulation.\\nBy combining large-scale AI models with sensory data, OpenAI aims to create systems that can reason and operate outside digital environments. The recruitment of humanoid robotics experts also indicates long-term goals that go beyond automation and toward robots that can work safely alongside people.11\\nMicrosoft’s report on early experiments with GPT-4\\nMicrosoft Research studied an early version of OpenAI’s GPT-4 in 2023. The report claimed that it showed greater general intelligence than previous AI models, performing at a human level in areas like math, coding, and law. This sparked debate on whether GPT-4 was a preliminary form of artificial general intelligence. 12\\nThe road to artificial general intelligence report by MIT\\n“The road to artificial general intelligence” report in August 2025 anticipates that early AGI-like systems could begin emerging between 2026 and 2028, showing human-level reasoning within specific domains, multimodal capabilities across text, audio, and physical interfaces, and limited goal-directed autonomy.\\nThe report combines aggregated forecasts and suggests a 50% probability that several generalized milestones, such as knowledge transfer and broad reasoning, will be achieved by 2028.\\nLonger-range projections estimate that machines may surpass human performance in all economically valuable tasks by around 2047, contingent on advances in compute efficiency, algorithmic breakthroughs, and autonomous learning.13\\nAI Frontiers on AGI probabilities\\nAdam Khoja and Laura Hiscott from AI Frontiers, a platform for AI debates and dialogues, estimate a 50% probability of reaching AGI by 2028 and an 80% probability by 2030, using their quantitative AGI definition.14\\nKhoja and Hiscott evaluate progress toward artificial general intelligence using a definition developed by Khoja, Dan Hendrycks, and their co-authors.15 Their framework measures ten cognitive abilities and assigns GPT-4 a score of 27% and GPT-5 a score of 57%. This indicates that current models are roughly halfway to the defined AGI threshold.\\nKhoja and Hiscott argue that traditional discussions about AGI timelines lack precision because they rely on inconsistent definitions. Their standardized framework is intended to create clarity by identifying specific strengths and weaknesses in current models. They note that reading, writing, mathematics, and general knowledge already meet or exceed human baselines and are no longer limiting factors.\\nThe authors highlight remaining gaps in visual reasoning, intuitive physics, auditory processing, perception-dependent speed, and visual and auditory working memory. They report rapid improvement on benchmarks such as SPACE and MindCube and suggest these gaps can likely be addressed through continued incremental research. They also observe that hallucinations remain a concern but are tractable given performance differences across leading models.\\nAccording to Khoja, Hiscott, and Hendrycks, the most significant remaining obstacle is continual learning and long-term memory storage. Current systems cannot retain information across sessions, and resolving this limitation will require at least one meaningful breakthrough. However, the authors emphasize that major AI labs are now prioritizing this area.\\nCommunity insights\\nWe also evaluated Metaculus community predictions on AGI, which involved the predictions of more than 3,290 participants:\\nInsights from AI entrepreneurs & individual researchers\\nAI entrepreneurs are also making estimates on when we will reach singularity, and they are more optimistic than researchers. This is expected as they benefit from increased interest in AI.\\nHere are the predictions of 13 of the most prominent AI entrepreneurs and researchers:\\nLearning from past over-optimism in AI predictions\\nKeep in mind that AI researchers were over-optimistic before. Examples include:\\nThis historical experience contributed to most current scientists shying away from predicting AGI in bold time frames like 10-20 years, but this has changed with the rise of generative AI.\\nUnderstand what singularity is\\nArtificial intelligence scares and intrigues us. Almost every week, there’s a new AI scare on the news, like developers afraid of what they’ve created or shutting down bots because they got too intelligent.34\\nMost of these myths result from research misinterpreted by those outside the AI and GenAI fields. Some stakeholders claim to fear AI because they may profit from more regulation, or it may bring them more attention.\\nThe greatest fear about AI is singularity (also called Artificial General Intelligence or AGI), which is an event that is expected to bring a rapid increase in machine intelligence. This is expected when a system combines human-level thinking with superhuman speed and rapidly accessible, near-perfect memory. According to some experts, singularity also implies machine consciousness.\\nSuch a machine could self-improve and surpass human capabilities. Even before artificial intelligence was a computer science research topic, science fiction writers like Asimov were concerned about this. They were devising mechanisms (i.e., Asimov’s Laws of Robotics) to ensure the benevolence of intelligent machines, which is more commonly called alignment research today.\\nWhy experts believe AGI is inevitable: Key arguments & evidence\\nReaching AGI may seem like a wild prediction, but it seems like quite a reasonable goal when you consider these facts:\\nConsidering that our intelligence is fixed and machine intelligence is growing, it is only a matter of time before machines surpass us unless there’s some hard limit to their intelligence. We haven’t encountered such a limit yet.\\nRecent achievements\\nOn August 7, 2025, OpenAI announced GPT-5, the most developed GPT model to date.\\nGPT-5 represents a strong step forward in narrow AI, with OpenAI promoting it as “PhD-level” in reasoning, coding, and writing, while also reducing hallucinations and exhibiting a more human-like demeanor.\\nSam Altman, OpenAI’s CEO, described it as the first model that feels like talking to an expert on any topic. Yet, critics urge caution. Prof. Carissa Véliz of Oxford’s Institute for Ethics in AI argues that systems like GPT-5 only mimic human reasoning rather than truly emulate it, warning that the hype may outpace reality.\\nSimilarly, Gaia Marcus of the Ada Lovelace Institute stresses that as models become more powerful, regulation lags dangerously behind.36\\nThis tension underscores GPT-5’s significance: it is a powerful evolutionary step that enhances reliability and usefulness, but it also shows the limits of current AI approaches and how far the field still is from achieving genuine AGI.37\\nAnother example is DeepMind’s Gemini in Deep Think mode, which achieved gold-medal performance at the 2025 International Mathematical Olympiad, marking a significant step in AI’s ability to reason through complex problems.\\nOperating entirely in natural language, Gemini solved five out of six problems within the official 4.5-hour contest window, while producing clear, human-readable proofs without relying on formal symbolic tools.\\nIts capabilities stem from several innovations: Deep Think mode enables parallel exploration of solution paths, training incorporates expert-level mathematical proofs, and reinforcement learning refines its strategic approach.\\nThis progress demonstrates that advanced AI can now engage in sophisticated, interpretable reasoning at a level once reserved for top human problem-solvers.38\\nExponential growth\\nThe following is a helpful analogy for understanding exponential growth. While machines may not seem highly intelligent right now, they can become quite smart in the near future.\\nRecent growth in AI computing capabilities\\nFigure 2: The figure shows a summary of the compute growth patterns observed across various categories: overall notable models (top left), frontier models (top right), leading language models (bottom left), and top models from leading companies (bottom right).\\nComputational resources for training AI models have significantly increased, with about two-thirds of language model performance attributed to model scale improvements.\\nAccording to a 2024 article,39 the growth of compute usage in training AI models has consistently increased by around 4-5x per year, reflecting trends in notable models, frontier models, and top companies like OpenAI, Google DeepMind, and Meta AI (See Figure 2).\\nHowever, the growth rate has slowed somewhat since 2018, especially for frontier models, but language models have experienced faster growth up to 9x/year until mid-2020, after which the pace slowed to 4-5x/year.\\nThe overall trend for AI compute growth remains strong, and projections suggest that the growth rate of 4-5x/year will continue unless new challenges or breakthroughs occur. This growth is also seen in the scaling strategies of leading AI companies, though slight variations exist between them.\\nDespite a slowdown in frontier model growth, the larger models released today, such as GPT-4 and Gemini Ultra, align closely with the predicted growth trajectory.\\nIf classic computing slows, quantum computing may fill the gap\\nClassic computing has taken us quite far. AI algorithms on classical computers can exceed human performance in specific tasks like playing chess or Go. For example, AlphaGo Zero beat AlphaGo by 100-0. AlphaGo had beaten the best players on earth.40 However, we are approaching the limits of how fast classical computers can be.\\nMoore’s law, which is based on the observation that the number of transistors in a dense integrated circuit doubles about every two years, implies that the cost of computing halves approximately every 2 years.\\nOn the other hand, most experts believe that Moore’s law is coming to an end during this decade.41  However, there are efforts to keep improving the efficiency of computing.\\nFor example, DeepSeek surprised global markets with its R1 model by delivering a reasoning model at a fraction of the cost of its competitors, like OpenAI.\\nQuantum Computing, which is still an emerging technology, can contribute to reducing computing costs after Moore’s law comes to an end. Quantum Computing is based on the evaluation of different states at the same time, whereas classical computers can calculate one state at a time.\\nThe unique nature of quantum computing can be used to efficiently train neural networks, currently the most popular AI architecture in commercial applications. AI algorithms running on stable quantum computers have a chance to unlock the singularity.\\nWhy do some experts believe that we will not reach AGI?\\nThere are 3 major arguments against the importance or existence of AGI. We examined them along with their common rebuttals:\\n1- Intelligence is multi-dimensional\\nTherefore, AGI will be different, not necessarily superior to human intelligence.\\nThis is true, and human intelligence is also different from animal intelligence. Some animals are capable of mental feats, like squirrels remembering where they hid hundreds of nuts for months.\\nYann LeCun, one of the pioneers of deep learning, believes that we should retire the word AGI and focus on achieving “advanced machine intelligence”.42 He argues the human mind is specialized and intelligence is a collection of skills and the ability to learn new skills. Each human can only accomplish a subset of human intelligence tasks.43\\nIt is also hard to understand the specialization level of the human mind, as humans, since we don’t know and can’t experience the entire spectrum of intelligence.\\nIn areas where machines exhibited super-human intelligence, humans were able to beat them by leveraging machine-specific weaknesses. For example, an amateur was able to beat a Go program that is on par with Go programs that beat world champions by studying and leveraging the program’s weaknesses.44\\n2- Intelligence is not the solution to all problems\\nScience\\nEven the best machine analyzing existing data may not be able to find a cure for cancer. It may need to run real-world experiments and analyze results to discover new knowledge in most areas.\\nMore intelligence can lead to better-designed and managed experiments, enabling more discovery per experiment. The history of research productivity should demonstrate this, but the data is quite noisy, and there are diminishing returns on research. We encounter harder problems like quantum physics as we solve simpler problems like Newtonian motion.\\nFinally, perfect predictions may not be possible in some domains due to the inherent randomness or immeasurability of that domain. For example, even with a wealth of data, we are not able to predict certain life outcomes with a high level of accuracy.45\\nEconomy\\nIntelligence is not the only ingredient for economic value generation.\\nFigure 3: IQ is correlated with wealth at low levels of wealth.46\\nFigure 4: IQ is not correlated with wealth if we only focus on high levels of wealth. This graph is the same as the one above except that net income levels below $40k have been hidden47\\n3- AGI is not possible because it is not possible to model the human brain\\nTheoretically, it is possible to model any computational machine, including the human brain, with a relatively simple machine that can perform basic computations and access infinite memory and time. This is the universally accepted Church-Turing hypothesis laid out in 1950. However, as stated, it requires certain difficult conditions: infinite time and memory.\\nMost computer scientists believe that modeling the human brain will take less than infinite time and memory. Nonetheless, there is no mathematically sound way to prove this belief, because we do not yet understand the brain well enough to precisely characterize its computational power. We will have to build such a machine!\\nHow can we reach AGI?\\nFigure 5: The time horizon of frontier AI models over time shows the longest tasks (in human-equivalent time) each model can complete with 50% reliability.48\\nThe above figure shows how AI agents’ capabilities have progressed over time by measuring the longest tasks they can complete with 50% reliability.\\nThe key finding is that the task length frontier models can handle has grown exponentially, doubling roughly every seven months. This means newer models, like Claude 3.7 Sonnet and o1, can now complete tasks that would take a human nearly an hour, while older models like GPT-2 could barely handle tasks longer than a few seconds.\\nThe shaded region reflects statistical uncertainty, but the overall trend is reliable. If this pattern continues, AI systems could soon handle complex tasks that take humans days or even weeks, marking a significant step toward broader autonomy and AGI-like capabilities.\\nScaling as a pathway to AGI\\nLeaders of frontier AI labs believe that scaling current transformer-based approaches can yield AGI, which fuels their predictions about achieving AGI in a few years.\\nOne proposed pathway to AGI is scaling up existing architectures like transformers by increasing compute and data, while another is developing entirely new approaches.\\nIn support of the scaling hypothesis, a 2024 report by Epoch AI analyzed whether AI compute growth can continue through 2030.\\nThey identified four major constraints: power availability, chip manufacturing capacity, data scarcity, and processing latency (See Figure 6).\\nDespite these challenges, they argue it’s feasible to train models requiring up to 2e29 FLOPs by the end of the decade, assuming significant investments in infrastructure.\\nSuch advancements could produce AI systems far more capable than today’s state-of-the-art models like GPT-4, pushing us closer to AGI.49\\nFigure 6: The chart illustrates the estimated upper bounds on AI training compute by 2030 under key constraints, power, chip production, data, and latency, with medians ranging from 2e29 to 3e31 FLOP.\\nBeyond scaling: The case for new architectures\\nHowever, influential AI scientists like Yann LeCun and Richard Sutton believe that scaling large language models will not lead to human-level intelligence.50 51  They believe that new architectures or approaches are necessary for AGI.\\nHow can we measure whether we have reached AGI?\\nLarge language models are blowing past new benchmarks every week, but evaluating LLMs is difficult due to issues like data poisoning and the lack of an accepted scientific definition for human-level intelligence.\\nThese concerns are amplified by insights from recent research52 which highlight that scaling LLMs is not a sustainable path to better performance, especially in scientific and high-stakes domains. The authors show that:\\nThese findings call into question the reliability of standard benchmarks and underscore the need for more diverse and evolving evaluation strategies.\\nOld metrics like the Turing test are no match for today’s machines, and new metrics like ARC-AGI may lack the generalization capabilities of broader benchmarks.\\nEmerging metrics like ARC-AGI aim to test abstraction and generalization, but may still lack resilience to data contamination or overfitting.\\nMoreover, as the paper highlights, even “good” loss scores may mask underlying information catastrophes due to non-Gaussian fluctuations and training instabilities.53\\nHow can we track the progress of LLMs?\\nThere are a few approaches to benchmarking to overcome these challenges:\\nWhat are approaches beyond benchmarking to determine AGI?\\nThere are potentially strong but lagging indicators of the impact of AI, which can help identify AGI.\\nEconomic growth\\nMicrosoft CEO Satya Nadella claims that 10% growth in the developed world would indicate AGI.54 . However, his incentive is to have a delayed definition of AGI since AGI would end OpenAI and Microsoft’s exclusive partnership.55\\nUnemployment\\nWe expect AGI to\\nIn a world where machines are more intelligent and efficient than humans, it wouldn’t be rational to pay a human to sit in front of a computer. Therefore, we expect white collar employment to plummet while humans continue to thrive in jobs in the physical world.\\nGovernment agencies collecting labor statistics classify jobs into detailed categories, making white collar employment an easy-to-track metric.\\nWe gathered data from the U.S. Bureau of Labor Statistics on white collar employment spanning 2019 to 2024.56 For clarity and consistency, we categorized white collar workers into the following occupational groups:\\nAccording to our analysis, the ratio of white collar workers to total employment has fluctuated between 45% and 48% over this period.\\nWhile this range suggests relative stability in the share of white collar employment so far, it is not indicative of a long-term trend, and we expect more pronounced shifts in the coming years as automation and AI adoption accelerate. For more predictions on how AI will change white collar and entry-level employment, read AI job loss.\\nShall we even aim for AGI?\\nThere are computer scientists who warn that focusing on AGI as the ultimate goal may distort AI research.57  Criticisms include: Creating an illusion of consensus, overfitting benchmarks, ignoring embedded social values, letting hype dictate priorities, building up “generality debt” (postponing key design questions), and excluding marginalized communities and under-resourced researchers.\\nSpecific, measurable, and transparent goals would be better for progress in AI than a vaguely defined goal like AGI.\\nMathematical reasoning behind AGI predictions\\nMathematical reasoning is central to understanding and forecasting AGI timelines. Many projections are based on quantifiable trends and formal models that guide expectations about when artificial general intelligence might emerge.\\nScaling laws and compute growth\\nOne key component of mathematical reasoning involves analyzing scaling laws. These show that model performance improves predictably with more data, parameters, and compute.\\nThe consistent 4–5× annual growth in AI training compute supports forecasts that AGI may be achievable within one or two decades, assuming current trends continue.\\nThese projections are based on empirical fits to performance curves and extrapolations, underpinned by power-law relationships, a core concept in mathematical modeling.\\nProbabilistic forecasting\\nResearchers also apply probabilistic methods to AGI predictions. Surveys often ask experts to estimate the probability of AGI being developed by specific years, producing cumulative probability distributions.\\nFor example, a 50% probability by 2040 reflects consensus under uncertainty, driven by Bayesian-style updating based on observed AI progress.\\nThis mathematical reasoning approach captures expert uncertainty without requiring precise dates, allowing ongoing revision as new data becomes available.\\nTheoretical foundations\\nThese forecasts are based on theoretical elements of mathematical reasoning, including the Church-Turing thesis, which implies that human cognition can be simulated by machines, and concepts like Kolmogorov complexity, which relate intelligence to the compressibility of information.\\nWhile such theories do not guarantee AGI, they provide a framework for thinking about its possibility and the computational requirements involved.\\nMore about Artificial General Intelligence\\nVideos from leading AI scientists:\\nDavid Silver, Principal Research Scientist at Google DeepMind\\nHe explains that Artificial General Intelligence (AGI) refers to AI systems capable of learning and excelling at a wide range of tasks; much like humans who can become experts in diverse fields such as science, music, or sports.\\nUnlike narrow AI limited to a single function, AGI aspires to mirror human adaptability and general problem-solving ability.\\nHe notes that while AGI is a long-term goal, reaching true human-level intelligence will likely require several breakthroughs and will develop gradually over time (See the video below).\\nIlya Sutskever, co-founder and Chief Scientist of OpenAI\\nIn the TED Talk “The Exciting, Perilous Journey Toward AGI,” he explores the rapid progress toward Artificial General Intelligence (AGI).\\nHe predicts AGI could emerge within the next 5 to 10 years, though he acknowledges uncertainty in this timeline.\\nSutskever highlights both the immense potential and the profound risks of AGI, stressing the need to align its development with human values. Despite the challenges, he is optimistic that humanity can safely guide this powerful technology (See the video below).\\nRay Kurzweil, computer scientist and entrepreneur\\nHe reflects on over six decades of AI progress, tracing humanity’s ability to build intelligence-enhancing tools, from primitive implements to large language models.\\nHe also predicts that Artificial General Intelligence will arrive by 2029, leading to technological singularity by 2045. He highlights exponential advances in computing power, medicine, and biotechnology.\\nHe also forecasts breakthroughs like AI-generated cures, digital clinical trials, and longevity escape velocity, where scientific progress could extend life indefinitely (See the video below).\\nYann LeCun, Turing award recipient\\nSee why LLMs can not give us human-level intelligence and the latest AI approaches to get there:\\n💡Conclusion\\nPredictions for AGI have shifted notably in recent years. While earlier surveys placed its arrival closer to 2060, recent forecasts, especially from entrepreneurs, suggest it could emerge as early as 2026–2035.\\nThis change is fueled by rapid advances in large language models and growing compute power. Yet, despite these gains, today’s AI still lacks the general flexibility and autonomy associated with human-level intelligence.\\nExperts remain divided on how AGI will be achieved; some believe scaling current architectures will be enough, while others argue that new methods are needed.\\nKey challenges include high resource demands, unclear benchmarks, and unresolved ethical concerns. AGI may be closer than ever, but its arrival still hinges on both technical breakthroughs and careful oversight.\\nFAQ\\nReference Links\\n\\nComments 12\\nShare Your Thoughts\\nYour email address will not be published. All fields are required.\\n\\nDoes anyone know when this article was first published? I want to do a comparison of predictions vs reality for a project.\\n\\nHi Harper.\\nThe article was first published in mid-2017. But it\\'s undergone constant updates since then to reflect the latest developments.\\nGood luck with your project and let us know if we can help further!\\n\\nI think we are far away from the point of singularity.\\nIt is not only that intelligence is multi dimensional, but also what is deemed as being intelligent (e.g., IQ, EQ) changes with time.\\nPeople also change with time.\\nSo what is that point of singularity may change.\\n\\nHello, Yuvan. Thank you for your feedback.\\n\\nHello,\\nAchieving the singularity from where we are now is relatively a simple jump, it is just time and advancements combined with a team somewhere who is dedicated to it and has the money to pull it off. The missing part of the equation would be asking the question \"what is consciousness?\" and understanding that. Then, understanding how to model that with non-biological machinery even at small levels, like modeling the consciousness of an amoeba or more advanced things like snakes and squirrels. Then if we know for certain what it is and how to model it, just run an adaptive evolution algorithm on itself, modeling out all of the processes in human cognition until it can beat them everywhere. Then, allow it to simply rebuild itself to continuously improve.\\nThe problem currently preventing this, is that human beings have no idea what consciousness is at all. It is a great mystery. One person thinks it is in the brain. Another thinks the brain is like a tuning fork, channeling the consciousness from somewhere else. It is a great mystery in science. When this problem is solved, then machine consciousness can be built most likely, depending on what it actually is.\\nIf consciousness is something weird, such as \"human beings have spirits in other dimensions that are planned for their bodies by a supreme being. The brain creates a quantum resonant frequency that links it together with this already conscious entity, and then several universes are interacting simultaneously to create the actual experience of being self aware and sentient\" well then, it will be very difficult to design a machine that does that same thing. It is more likely that we figure out how to model the resonance in the brain and then transfer an already existing consciousness of an animal or a human into a machine and keep it going, if that even makes any sense at all.\\nHowever, maybe that\\'s not how it works, and it is something simple like the holographic connection of energy patterns fluctuating in the mind - this can be modeled and a machine can be built that does these sorts of things with much more efficiency. Right now the mystery of the problem is consciousness itself.\\nHope that helps. I really enjoyed the robot soccer tournament. I also feel like a superhero at soccer now.\\n\\nIt\\'s becoming clear that with all the brain and consciousness theories out there, the proof will be in the pudding. By this I mean, can any particular theory be used to create a human adult level conscious machine. My bet is on the late Gerald Edelman\\'s Extended Theory of Neuronal Group Selection. The lead group in robotics based on this theory is the Neurorobotics Lab at UC at Irvine. Dr. Edelman distinguished between primary consciousness, which came first in evolution, and that humans share with other conscious animals, and higher order consciousness, which came to only humans with the acquisition of language. A machine with primary consciousness will probably have to come first.\\nThe thing I find special about the TNGS is the Darwin series of automata created at the Neurosciences Institute by Dr. Edelman and his colleagues in the 1990\\'s and 2000\\'s. These machines perform in the real world, not in a restricted simulated world, and display convincing physical behavior indicative of higher psychological functions necessary for consciousness, such as perceptual categorization, memory, and learning. They are based on realistic models of the parts of the biological brain that the theory claims subserve these functions. The extended TNGS allows for the emergence of consciousness based only on further evolutionary development of the brain areas responsible for these functions, in a parsimonious way. No other research I\\'ve encountered is anywhere near as convincing.\\nI post because on almost every video and article about the brain and consciousness that I encounter, the attitude seems to be that we still know next to nothing about how the brain and consciousness work; that there\\'s lots of data but no unifying theory. I believe the extended TNGS is that theory. My motivation is to keep that theory in front of the public. And obviously, I consider it the route to a truly conscious machine, primary and higher-order.\\nMy advice to people who want to create a conscious machine is to seriously ground themselves in the extended TNGS and the Darwin automata first, and proceed from there, by applying to Jeff Krichmar\\'s lab at UC Irvine, possibly. Dr. Edelman\\'s roadmap to a conscious machine is at \\n\\nI think Patrick Winston was joking when he said 20 years. From the linked quote:\\n\"I was recently asked a variant on this question. People have been saying we will have human-level intelligence in 20 years for the past 50 years. My answer: I’m ok with it. It will be true eventually.\"\\n\"Forced into a corner, with a knife at my throat, I would say 20 years, and I say that fully confident that it will be true eventually.\"\\n\\nGreat point! We should have read the source more carefully. I tried to explain his point better in the article.\\n\\nI have the impression that the nerds that make this kind of prediction (replicate human brain) know a whole lot about computer programming but are ignorant about neuroscience/psychology. We are nor even scratching the surface about primary phenomenon, such as counsciousness / unconsciousness. How do you claim that you can replicate something that we are still far from understanding how it works?\\n\\nThank you for the comment. True, better understanding of the mind would help AGI research.\\n\\nmmm... I\\'m not sure we can reach to this point: \"benevolence of intelligent machines\" Emotions and Feelings are there to guide our actions, to improve ourselves and to make a better world, can we make a machine to feel guilt of being smarter than us??\\n\\nSaying human intelligence is fixed ignores that as we learn more about how the human brain works we may learn how to expand its capability\\'s ie through some form of enhanced learning, targeted drugs, gene therapy, electro stimulation and not just direct brain computer connections being the only potential for doing this. More so currently hampered by our lack of understanding even the language you use has an effect on your cognitive ability\\'s its one of the reasons deaf people were called dumb was the occurrence of language deprivation and how it negatively effected neurodevelopment it was a major problem when deaf children were forced to lip read instead of using sign language .\\nBut we will need more powerful AIs to achieve an understanding of our brains\\n\\nPeople who say AGI will be here in 2060 are idiots and don\\'t understand the flow of technology you\\'ll see\\n\\n@Vyn What do you mean? Do you mean to say it will take way before or way after 2060?\\n\\nThanks! I\\'ll be quite happy if I get to see 2060\\n\\nIntelligent doesn\\'t solve our all problems maybe yes but certainly its essential and more intelligent you are faster you solve problems. If you are a chimp you can not even pour water to a glass. You do not even know what glass is used for. Yes if you are human being you still need to get up and grab the glass but intellegence is essential. I do not think human brain is impossible to create in a lab. I think earth is a lab. Anything found in nature can be replicate in the lab.\\n\\nif P=NP then the singularity may happen also.\\nSaying the human brain is impossible to recreate I dont agree with, but to say its intractable probably is approximately true. So P=NP, if you could solve that mystery (which is the millenial prize funnily) with an intractable calculation, that could make all the magic happen as well.\\n\\nThanks for the comment. Most computer scientists working on AI or machine learning would agree that it is possible to replicate human brain\\'s capabilities.\\n\\nThe claim that \"humans contribute most to the biomass\" on the planet is likely to be wrong. Check out this paper for a careful estimation:\\n\\nThank you! That was insightful. Biology is not my strong suit, I should stick to computer science.\\n\\n@AIMultiple Humble response, and great article. Thanks a ton :)\\n\\n@B Thanks!\\nWe follow ethical norms & our process for objectivity. AIMultiple\\'s customers in AI Foundations include Creatio.\\nNext to Read\\nBuilding AI Agents with Anthropic\\'s 6 Composable Patterns\\n\\nAI Agent Deployment: Steps and Challenges\\n\\nComparison of Top 5 AI Survey Tools\\n\\n8 AI Code Models Benchmarked: LMC-Eval\\n\\nAGI Benchmark: Can AI Generate Economic Value\\n\\nCompare 50+ AI Agent Tools', \"Site-wide navigation\\nTopics\\nTrending\\nTopics\\nResearch Divisions\\nRAND's divisions conduct research on a uniquely broad front for clients around the globe.\\nU.S. research divisions\\nInternational research divisions\\nThis report represents the ongoing policy debate on the race to artificial general intelligence (AGI) in a mathematically neutral model that allows policymakers to compare the outcomes of alternative strategies in international competition. The model suggests that incentives will continue to be aligned with accelerated development until coordination mechanisms are designed that are grounded in a common knowledge of the global risks of AGI.\\nA Prisoner’s Dilemma in the Race to Artificial General Intelligence\\nLisa Abraham, Joshua Kavner, Alvin Moon\\nResearchPublished Dec 1, 2025\\n\\nThe purpose of this report is to represent the ongoing policy debate on the race to artificial general intelligence (AGI) in a mathematically neutral model that allows policymakers to compare the outcome of alternative strategies in international technology competition. The analysis assumes that outcomes are driven by a strategic choice of whether to accelerate the development of AGI based on the trade-off between the perceived benefits of securing a first-mover advantage and the perceived risks, such as developing an uncontrolled or unaligned AGI, misuse by non-experts to create new weapons of mass destruction, instability between great powers that escalates to conflict, and other threats to human survival, flourishing, and global security. Although the model does not capture the full complexity of present-day racing dynamics, it suggests that incentives will continue to be aligned with accelerated development until coordination mechanisms are designed that are grounded in a common knowledge of the global risks of advancing toward AGI. This report is intended for policymakers and general audiences who are interested in understanding these racing dynamics and geopolitical implications.\\nKey Findings\\nIf both the United States and China have the same shared assessment that the benefits of being the first to achieve AGI outweigh the risks, they are effectively locked in a prisoner’s dilemma\\nIf a common shared assessment emerges that the risks of catastrophic harm from accelerated development of AGI exceed the potential benefits of a first-mover advantage, the incentives can be aligned toward cooperation\\nVerification methods that allow countries to credibly signal that they are cooperating can increase the chance that cooperation can be sustained\\nSubscribe to the Policy Currents newsletter\\nTopics\\nDocument Details\\nCitation\\nRAND Style Manual\\nChicago Manual of Style\\nResearch conducted by\\nThis research was independently initiated and conducted by the Center for the Geopolitics of Artificial General Intelligence with RAND Global and Emerging Risks using income from operations and gifts from RAND supporters, including philanthropic gifts.\\nThis publication is part of the RAND research report series. Research reports present research findings and objective analysis that address the challenges facing the public and private sectors. All RAND research reports undergo rigorous peer review to ensure high standards for research quality and objectivity.\\nThis document and trademark(s) contained herein are protected by law. This representation of RAND intellectual property is provided for noncommercial use only. Unauthorized posting of this publication online is prohibited; linking directly to this product page is encouraged. Permission is required from RAND to reproduce, or reuse in another form, any of its research documents for commercial purposes. For information on reprint and reuse permissions, please visit www.rand.org/pubs/permissions.\\nRAND is a nonprofit institution that helps improve policy and decisionmaking through research and analysis. RAND's publications do not necessarily reflect the opinions of its research clients and sponsors.\\n\\nRAND is a research organization that develops solutions to public policy challenges to help make communities throughout the world safer and more secure, healthier and more prosperous. RAND is nonprofit, nonpartisan, and committed to the public interest.\\nRAND Headquarters\\nP.O. Box 2138\\u2028\\n1776 Main Street\\nSanta Monica, CA 90401-2138\\nRAND has offices across the U.S., in Europe, and in Australia\\nSubscribe to the Policy Currents Newsletter\\nAnd stay on top of the issues that matter most.\\nRAND® is a registered trademark. © 2025 RAND Corporation.\\nPrivacy PolicyTerms and Conditions\\nThis site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.\"], 'referred_urls': ['https://www.arm.com/resources/report/mittr-artificial-general-intelligence', 'https://www.scientificamerican.com/article/how-close-are-todays-ai-models-to-agi-and-to-self-improving-into/', 'https://ai-frontiers.org/articles/agis-last-bottlenecks', 'https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/', 'https://www.rand.org/pubs/research_reports/RRA4245-1.html'], 'initial_draft': '<!doctype html>\\n<html>\\n<head>\\n  <meta charset=\"utf-8\">\\n  <title>Where We Stand on AGI: Latest Developments, Numbers, and Open Questions</title>\\n</head>\\n<body>\\n  <h1>Where We Stand on AGI: Latest Developments, Numbers, and Open Questions</h1>\\n\\n  <p>Artificial General Intelligence (AGI) — a machine that can think and learn across many domains like a human — is a hot topic. Different experts disagree about when (or if) it will arrive. Below I summarize the latest developments from recent reports and articles, explain the main technical bottlenecks, and give you the key numbers and arguments so you can see why timelines range from \"within a few years\" to \"decades away.\" I keep this friendly for high‑school and early college students.</p>\\n\\n  <h2>Quick snapshot of major recent headlines</h2>\\n  <ul>\\n    <li>OpenAI released GPT-5 (announced Aug 7, 2025) — presented as a big step up in reasoning, coding and multimodal support.</li>\\n    <li>Benchmarks and expert studies place current top models roughly “halfway” to some formal AGI definitions: one framework scored GPT-4 at 27% and GPT-5 at 57% toward an AGI threshold.</li>\\n    <li>Some industry/academic reports and panels (e.g., an MIT/Arm deep dive) warn AGI-like systems might show up as early as 2026; others and many surveys keep median predictions later (2040–2060 ranges).</li>\\n    <li>Policy and geopolitics matter: RAND (Dec 1, 2025) models the US–China AGI race as a prisoner’s dilemma — incentives favor speed unless international coordination and verification improve.</li>\\n  </ul>\\n\\n  <h2>How close are models? Numbers you should remember</h2>\\n  <p>People are trying to measure AGI with concrete tests. Here are some of the hard numbers from recent analyses and benchmarks:</p>\\n  <ul>\\n    <li>One ten-ability AGI framework (inspired by human intelligence theory) gave GPT-4 a 27% score and GPT-5 a 57% score.</li>\\n    <li>On SPACE (visual reasoning subset): GPT-4o scored 43.8%; internal tests reported GPT-5 (Aug 2025) at 70.8% while humans average 88.9%.</li>\\n    <li>On MindCube (spatial/working‑memory tests): GPT-4o = 38.8%, GPT-5 = 59.7% (still below human average).</li>\\n    <li>SimpleQA (a test for hallucination/accuracy): GPT-5 reportedly hallucinated in response to over 30% of questions; some models (e.g., Anthropic’s Claude variants) hallucinate much less.</li>\\n    <li>Self‑improvement endurance: METR’s test of GPT‑5.1‑Codex‑Max showed sustained autonomous task performance around 2 hours 42 minutes — a large jump from GPT‑4’s few minutes on similar tests.</li>\\n    <li>DeepMind’s Gemini (in “Deep Think” mode) scored gold at the 2025 International Mathematical Olympiad by solving 5 of 6 problems within the 4.5‑hour window — a sign of strong formal reasoning in controlled tasks.</li>\\n  </ul>\\n\\n  <h2>Where models still struggle (the real bottlenecks)</h2>\\n  <p>Researchers point to a few consistent gaps that stop today\\'s systems from being “general” in the human sense:</p>\\n  <ul>\\n    <li><strong>Continual learning / long‑term memory storage:</strong> Current models are usually frozen after training and don’t reliably learn from new interactions over weeks or months. Authors call this the single most uncertain remaining obstacle — it likely needs a breakthrough in how models update or store durable knowledge.</li>\\n    <li><strong>Multimodal perception (especially vision):</strong> Models are getting far better with text and math, but visual induction, world modeling (understanding physical plausibility), and some types of visual working memory lag behind humans.</li>\\n    <li><strong>Hallucinations / reliable retrieval:</strong> Many models confidently give wrong answers; reducing this is an active area and some models differ substantially in error rates.</li>\\n    <li><strong>Speed and real‑world tool use:</strong> Models are superfast in text but can be slower when perception or low‑latency actions are needed.</li>\\n  </ul>\\n\\n  <h2>How researchers think we’ll get from here to AGI</h2>\\n  <p>There are two broad camps:</p>\\n  <ol>\\n    <li><strong>Scale current methods:</strong> Some labs argue that scaling transformers, more compute and better data will push capabilities to AGI. Historically compute used to train big models grew roughly 4–5× per year (earlier bursts were as high as 9×/year until mid‑2020).</li>\\n    <li><strong>New architectures or breakthroughs:</strong> Others (e.g., Yann LeCun, Richard Sutton) say scaling won’t be enough — we’ll need new ideas (e.g., better world models, memory architectures, or robotics integration).</li>\\n  </ol>\\n  <p>Compute projections vary: one analysis (Epoch AI) suggested training budgets up to ~2e29 FLOPs are feasible by 2030 under some assumptions; other upper bounds in reports ranged up to ~3e31 FLOPs depending on power, chip production and data limits.</p>\\n\\n  <h2>Timelines: why predictions disagree so much</h2>\\n  <p>Different groups pick different metrics and weights. Surveys of thousands of experts show median guesses spanning decades (many 50%‑probability dates around 2040–2060), while some entrepreneurs and narrowly framed AGI definitions yield much earlier numbers (for example, one set of authors estimated a 50% chance by end‑2028 and 80% by end‑2030 under their framework).</p>\\n  <p>Some industry reports (e.g., MIT/Arm) even highlight that a minority of experts think AGI could appear as soon as 2026 — overall, the spread is wide because of differences in definitions and which bottlenecks each person thinks are tractable.</p>\\n\\n  <h2>Risks, governance and geopolitics</h2>\\n  <p>Progress isn’t just technical. There are lots of policy and safety questions:</p>\\n  <ul>\\n    <li>Geopolitics: RAND models the US–China race as a prisoner’s dilemma where both sides keep pushing acceleration unless credible verification and common risk judgments emerge.</li>\\n    <li>Security risks: papers warn of misuse (e.g., biological risk from expert-level virology outputs), espionage, and supply‑chain issues (chip export controls like H20/GPU debates matter).</li>\\n    <li>Safety strategies: researchers propose technical assurance, transparency, and even deterrence ideas (like \"Mutually Assured AI Malfunction\") — but these raise hard observability and verification problems.</li>\\n    <li>Ethics and law: debates about openness, liability, and whether to keep powerful models behind paywalls are active and unresolved.</li>\\n  </ul>\\n\\n  <h2>Bottom line for students</h2>\\n  <p>We’re seeing rapid, measurable progress: top models now match or beat humans in many narrow tasks, have much larger memory windows, and can sustain autonomous code‑writing for hours. But they still miss key things humans do easily — durable long‑term learning, consistent visual world modeling, and trustworthy truthfulness.</p>\\n  <p>Timelines are uncertain because they depend on whether the remaining gaps are solved by \"business‑as‑usual\" engineering, a single useful breakthrough (e.g., workable continual learning), or entirely new approaches. Meanwhile, policy and safety work is racing to catch up with technical progress.</p>\\n\\n  <p>If you’re curious, keep an eye on benchmark numbers (AGI‑score frameworks, SPACE, IntPhys, MindCube, SimpleQA), compute growth reports, major model releases (GPT‑5 and beyond), and policy studies like RAND’s — they tell the clearest stories about how close AGI really is.</p>\\n</body>\\n</html>', 'feedback': 'Overall impression — well organized, timely, and accessible. The report has a clear intent (summarize recent AGI developments for high‑school/early‑college readers), good topical coverage (headlines, benchmark numbers, bottlenecks, pathways, timelines, governance), and helpful short lists that make technical topics approachable. However, to be a robust, persuasive piece for readers who will use the numbers and claims as evidence, it needs clearer sourcing, tighter argumentation, and several structural and stylistic improvements.\\n\\nStructure (what works and what to fix)\\n- Strengths: Logical high‑level flow (headline snapshot → key numbers → bottlenecks → pathways → timelines → risks → bottom line). Clear section headings and short subsections make scanning easy.\\n- Weaknesses:\\n  - No executive summary or one‑line takeaway at the top (there is a “Bottom line for students,” but an explicit executive summary + date/version would help).\\n  - Missing a references / sources section. Many specific numbers and claims are presented without inline citations (benchmarks, dates, internal tests, RAND modeling, METR results).\\n  - Methods and provenance are unclear: how were scores aggregated (the “ten‑ability framework” and AGI score), what counts as “internal tests,” and how reliable are the reported percentages?\\n  - Mixing primary reporting (benchmarks) with interpretive content and opinion without signaling provenance or confidence.\\n- Recommendations:\\n  - Add a short executive summary with the main numeric claims and confidence levels.\\n  - Add a clearly formatted references section with links, dates, and short descriptions for every study/benchmark cited.\\n  - Add a brief “Methods and definitions” box that explains the AGI score, weighting, and what each benchmark measures.\\n\\nClarity and accessibility\\n- Strengths: Generally plain language, friendly tone targeted to the stated audience; lists and short paragraphs help comprehension.\\n- Weaknesses:\\n  - Jargon and unexplained terms appear (e.g., “SPACE,” “MindCube,” “METR,” “o1,” “Codex‑Max”) without quick explanations or hyperlinks. Readers unfamiliar with these benchmarks will be lost.\\n  - Some claims are presented as facts though they are contested (e.g., “GPT‑5 scored 57% toward AGI”) — readers need context on uncertainty and alternate estimates.\\n  - Inconsistent use of qualifiers: sometimes “reported” or “internal tests” is used, other times the text reads as authoritative.\\n- Recommendations:\\n  - For each benchmark or dataset, insert a one‑sentence explanation (what it measures, how it’s scored) and a link.\\n  - Use consistent qualifiers (e.g., “reported by [source],” “internal tests by [lab],” “peer‑reviewed study”) and give confidence ranges where possible.\\n  - Consider a short glossary for acronyms and benchmark names.\\n\\nStrength of argument and evidence\\n- Strengths: The report sensibly focuses on concrete bottlenecks (continual learning, multimodal perception, hallucinations) and outlines the two dominant pathways (scale vs new architectures). It correctly frames timelines as dependent on definitions and bottlenecks.\\n- Weaknesses:\\n  - Many decisive numeric claims lack provenance or methodology (e.g., the 27%/57% AGI scores, GPT‑5 Space/MindCube numbers, “GPT‑5 hallucinated in over 30% of questions”).\\n  - Heavy reliance on “internal tests” and single‑study estimates without discussing sampling bias, benchmark hygiene, data contamination, or reproducibility.\\n  - The argument that “only continual learning remains as a breakthrough” is presented as a near‑conclusion but rests on subjective weighting of capabilities; alternative views (need for new architectures) are acknowledged but not evaluated with equivalent depth.\\n  - Geopolitical and governance sections summarize risks but do not assess likelihoods or tradeoffs; policy recommendations are thin.\\n- Recommendations:\\n  - Where possible, present the source, the test methodology, sample size, and whether results are independently reproducible. If unavailable, mark the claim as provisional.\\n  - Include counterevidence and explicit uncertainty bands for timeline claims (e.g., median, interquartile range, who was surveyed).\\n  - Strengthen the governance section by citing one or two concrete policy proposals and their pros/cons (e.g., export controls, verification regimes, third‑party auditing).\\n  - If asserting that one bottleneck is dominant, show the analytic steps or scoring that lead to that conclusion (e.g., contribution of each capability to the final AGI score).\\n\\nWriting style and tone\\n- Strengths: Engaging, non‑alarmist, and approachable for the intended audience. Good use of bullets and short sentences.\\n- Weaknesses:\\n  - Occasional imprecise or hyperbolic phrasing (e.g., “scored gold at the 2025 IMO” — strong, but readers will want more nuance about task constraints and evaluation).\\n  - Some repetition (similar points about continual learning, benchmarks, and uncertainty appear in multiple sections).\\n  - Passive constructions and vague references (“one framework,” “internal tests,” “some industry reports”) reduce credibility.\\n- Recommendations:\\n  - Tighten language: reduce repetition, prefer active voice, and replace vague phrases with explicit attribution.\\n  - Use a consistent register: if aimed at high‑school/early‑college, keep explanations simple but add optional “read more” links for deeper technical detail.\\n  - Where strong claims are made, temper them with qualifiers or footnotes pointing to underlying uncertainty.\\n\\nPresentation of numbers and data\\n- Strengths: The report gives concrete percentages and times (helpful for readers who like numbers).\\n- Weaknesses:\\n  - Numbers are presented without margins of error, sample sizes, or dates of measurement (important because model updates change metrics rapidly).\\n  - Some big numerical claims (compute FLOP upper bounds, METR endurance times) lack citations or explanation of assumptions.\\n- Recommendations:\\n  - Add a small data table summarizing each benchmark: benchmark name, what it measures, date of evaluation, model tested, score, human baseline, source link.\\n  - Add brief notes on reproducibility and whether scores were independently validated.\\n\\nBalance and fairness\\n- Strengths: The report cites multiple viewpoints (scaling proponents vs new architecture proponents), mentions policy and existential risk debates, and acknowledges wide timeline variance.\\n- Weaknesses:\\n  - It sometimes privileges optimistic/industry‑friendly estimates by reporting precise percentages while treating skeptical positions more qualitatively.\\n  - Lacks clear separation between reporting (what various actors claim) and the author’s synthesis or opinion.\\n- Recommendations:\\n  - Explicitly label “claims” vs “synthesis”: e.g., a sidebar “What different camps claim” and another “My synthesis (with uncertainty).”\\n  - When presenting timelines, show a simple visualization or table of major survey medians vs optimistic industry forecasts.\\n\\nSpecific factual / editorial fixes\\n- Provide publication date and author information at top (the report references developments through late‑2025 but the document itself has no timestamp).\\n- Spell out benchmark names at first mention (e.g., SPACE — Visual Reasoning subset).\\n- Replace “internal tests reported GPT‑5 (Aug 2025) at 70.8%” with a citation and a note on whether results are publicly verifiable.\\n- Clarify what “AGI threshold” means numerically and philosophically; if using a 95% threshold in the framework, justify choice.\\n- Add a short paragraph discussing benchmark brittleness and data contamination risks (models sometimes overfit benchmarks if trained on similar datasets).\\n\\nPrioritized action list (quick wins)\\n1. Add an executive summary + publish date + authorship.\\n2. Insert a references list linking every benchmark, RAND/METR/AI Frontiers reports, and primary sources.\\n3. Add a one‑row summary table for each benchmark with source, date, model, and human baseline.\\n4. Add a methods/definitions box explaining the AGI score and weighting.\\n5. Temper precise numeric claims that rely on “internal” or single‑lab data; mark them provisional or provide verification options.\\n\\nFinal note\\nThe report is a useful primer that successfully communicates why AGI timelines vary and what technical bottlenecks matter. To make it reliable for policymakers, educators, and students who will act on its numbers, the author should prioritize transparent sourcing, explicit methodology, and clearer treatment of uncertainty. These changes will substantially raise the piece’s credibility without sacrificing accessibility.', 'final_draft': '<!doctype html>\\n<html>\\n<head>\\n  <meta charset=\"utf-8\">\\n  <title>Where We Stand on AGI: Latest Developments, Numbers, and Open Questions</title>\\n</head>\\n<body>\\n  <h1>Where We Stand on AGI: Latest Developments, Numbers, and Open Questions</h1>\\n  <p><strong>Author:</strong> Compiled summary (anonymous) · <strong>Date:</strong> December 2025 · <strong>Version:</strong> 2025-12-01</p>\\n\\n  <h2>Executive summary (one line)</h2>\\n  <p>Top models have made rapid, measurable gains (e.g., GPT‑5 reported around 50–70% on several AGI-oriented benchmarks), but persistent, hard-to-solve gaps — especially durable continual learning, robust multimodal world models, and reliable truthfulness — mean credible AGI timelines still range from a few years (for narrow definitions) to several decades (for robust human‑level generality). Numbers below are reported by labs and studies; where results come from internal tests or single groups I flag them as provisional.</p>\\n\\n  <h2>Quick snapshot of major recent headlines</h2>\\n  <ul>\\n    <li>OpenAI released GPT‑5 (announced Aug 7, 2025) — presented as a notable step up in reasoning, coding and multimodal support (press release and model paper reported improvements).</li>\\n    <li>Benchmarks and expert studies place current top models roughly “halfway” to some formal AGI definitions: a ten‑ability AGI framework reported GPT‑4 at 27% and GPT‑5 at 57% toward its chosen AGI threshold (framework authors’ reported scores).</li>\\n    <li>Some industry/academic reports and panels (for example, an MIT/Arm deep dive) warn AGI‑like systems might appear as early as 2026; other expert surveys keep median predictions later (many 50%‑probability dates clustered around 2040–2060).</li>\\n    <li>Policy and geopolitics matter: RAND (modeling reported Dec 1, 2025) frames the US–China AGI race as a prisoner’s dilemma — incentives favor speed absent stronger international coordination and verification.</li>\\n  </ul>\\n\\n  <h2>Methods and definitions (short)</h2>\\n  <p><strong>What “AGI score” means here:</strong> the draft uses several benchmarking frameworks that combine multiple task categories (reasoning, planning, perception, memory, tool use). Each framework weights abilities differently and maps aggregate performance to a 0–100% scale relative to an internal \"AGI threshold\" chosen by its authors. These mappings are normative — not universally agreed — so percentages should be read as framework‑specific progress indicators, not absolute measures of human‑level general intelligence.</p>\\n  <p><strong>Provenance notes:</strong> I flag results as (a) published/peer‑reviewed, (b) public benchmark results, or (c) reported/internal tests by labs. Where items are internal or single‑lab reports they are provisional and should be independently verified before being used as firm evidence.</p>\\n\\n  <h2>Benchmarks and headline numbers (compact table)</h2>\\n  <table border=\"1\" cellpadding=\"6\" cellspacing=\"0\">\\n    <tr>\\n      <th>Benchmark</th><th>What it measures</th><th>Model / Score</th><th>Human baseline / Notes</th><th>Source type</th>\\n    </tr>\\n    <tr>\\n      <td>Ten‑ability AGI framework</td>\\n      <td>Aggregate across ~10 cognitive abilities</td>\\n      <td>GPT‑4: 27% · GPT‑5: 57%</td>\\n      <td>Framework‑specific AGI threshold (authors\\' mapping)</td>\\n      <td>Reported framework scores (authors)</td>\\n    </tr>\\n    <tr>\\n      <td>SPACE (visual reasoning subset)</td>\\n      <td>Visual reasoning tasks (subset)</td>\\n      <td>GPT‑4o: 43.8% · GPT‑5 (Aug 2025): 70.8%</td>\\n      <td>Human average: 88.9%</td>\\n      <td>Internal/public benchmark reports (reported)</td>\\n    </tr>\\n    <tr>\\n      <td>MindCube</td>\\n      <td>Spatial / working‑memory tests</td>\\n      <td>GPT‑4o: 38.8% · GPT‑5: 59.7%</td>\\n      <td>Still below typical human average</td>\\n      <td>Benchmark reports (reported)</td>\\n    </tr>\\n    <tr>\\n      <td>SimpleQA</td>\\n      <td>Hallucination / factual accuracy</td>\\n      <td>GPT‑5: hallucinations in >30% of questions (reported)</td>\\n      <td>Some other models (e.g., Anthropic Claude variants) report lower hallucination rates</td>\\n      <td>Reported / model vendor comparisons</td>\\n    </tr>\\n    <tr>\\n      <td>METR endurance test</td>\\n      <td>Sustained autonomous task performance</td>\\n      <td>GPT‑5.1‑Codex‑Max: ~2 hours 42 minutes · GPT‑4: few minutes</td>\\n      <td>Measures autonomous chaining and robustness over time</td>\\n      <td>Internal lab test (provisional)</td>\\n    </tr>\\n    <tr>\\n      <td>IMO 2025 (DeepMind Gemini, \"Deep Think\" mode)</td>\\n      <td>Formal math problem solving under contest constraints</td>\\n      <td>Solved 5 of 6 problems within 4.5 hours (gold‑level performance reported)</td>\\n      <td>Shows strong formal reasoning in a constrained task</td>\\n      <td>Reported by DeepMind (lab result)</td>\\n    </tr>\\n  </table>\\n\\n  <h2>Where models still struggle (the real bottlenecks)</h2>\\n  <ul>\\n    <li><strong>Continual learning / long‑term memory:</strong> Most models remain effectively \"frozen\" after training; reliably updating and storing durable knowledge over weeks/months remains unsolved and is widely cited as a high‑uncertainty obstacle.</li>\\n    <li><strong>Multimodal perception (vision & world models):</strong> Text and math abilities have improved faster than visual induction and physical‑world modeling; visual working memory and physical plausibility judgments still lag humans.</li>\\n    <li><strong>Hallucinations and reliable retrieval:</strong> High‑confidence errors persist (SimpleQA >30% hallucination reported for GPT‑5 in one test); different model families show substantial variance.</li>\\n    <li><strong>Low‑latency tool use & situated action:</strong> Language is fast; perception‑action loops and real‑world tool use (robotics) remain harder and slower.</li>\\n  </ul>\\n\\n  <h2>How researchers think we’ll get from here to AGI</h2>\\n  <p>Two broad routes dominate discussion:</p>\\n  <ol>\\n    <li><strong>Scale current methods:</strong> Proponents argue more parameters, compute and better data will continue yielding returns. Historical training‑compute growth averaged ~4–5×/year (with earlier bursts up to ~9×/year until mid‑2020).</li>\\n    <li><strong>New architectures / breakthroughs:</strong> Others (e.g., prominent ML researchers) argue scaling alone won’t close key gaps and that innovations (robust world models, persistent memory systems, tighter robotics integration) are needed.</li>\\n  </ol>\\n  <p>Compute projections vary: one analysis (Epoch AI) suggested training budgets up to ~2×10^29 FLOPs could be feasible by 2030 under optimistic assumptions; other reports place upper bounds near ~3×10^31 FLOPs depending on power and chip production assumptions.</p>\\n\\n  <h2>Timelines: why predictions disagree</h2>\\n  <p>Different metrics, definitions and confidence levels drive wide disagreement. Aggregated expert surveys show medians often in the 2040–2060 range, while some narrow frameworks and industry estimates give earlier dates (one internal framework estimated 50% by end‑2028 and 80% by end‑2030 under its assumptions). A minority of experts and some industry reports have suggested AGI‑like capabilities could appear as early as 2026. When using these numbers, note the underlying definition of AGI, which benchmark(s) are weighted most heavily, and whether the estimate is conditional on continued scaling or a specific breakthrough.</p>\\n\\n  <h2>Risks, governance and geopolitics</h2>\\n  <ul>\\n    <li><strong>Geopolitics:</strong> RAND models (Dec 1, 2025 reporting) show a prisoner’s dilemma: nations face incentives to accelerate unless international verification and shared risk assessments improve.</li>\\n    <li><strong>Security risks:</strong> Reports warn of misuse (e.g., advances in bio‑expertise outputs), espionage, and supply‑chain chokepoints (chip export controls and debates around GPU access matter for pace of progress).</li>\\n    <li><strong>Safety strategies:</strong> Proposals range from technical assurance and transparency to verification regimes and deterrence ideas; all face verification and observability challenges.</li>\\n    <li><strong>Ethics and law:</strong> Active debates continue over openness, liability, and model access control (paywalls vs open releases).</li>\\n  </ul>\\n\\n  <h2>Bottom line for students (and what to watch)</h2>\\n  <p>Progress is real and measurable: top models now match or beat humans on many narrow tasks, have larger context windows, and can sustain autonomous code writing for hours in some internal tests. But key human‑like capacities — durable continual learning, reliable multimodal world models, and trustworthy factuality — remain outstanding. Timelines hinge on whether these gaps are closed by continued scaling, a single breakthrough (e.g., workable continual learning), or new architectures. Policy and safety research must accelerate in parallel.</p>\\n  <p>Watch these signals: AGI‑score framework updates, SPACE / IntPhys / MindCube / SimpleQA benchmark results, compute growth analyses (e.g., Epoch AI), major model releases (GPT‑5 and successors), METR endurance reports, and policy studies like RAND’s — and when possible, prioritize independently reproducible benchmark results over single‑lab internal tests.</p>\\n\\n  <h2>References and sources (brief)</h2>\\n  <ul>\\n    <li>OpenAI GPT‑5 announcement — Aug 7, 2025 (model release/press materials; reported performance claims).</li>\\n    <li>Ten‑ability AGI framework — authors’ reported scores for GPT‑4 (27%) and GPT‑5 (57%) (framework paper/report; framework‑specific mapping to AGI threshold).</li>\\n    <li>SPACE visual reasoning subset results — reported GPT‑4o 43.8%, GPT‑5 (Aug 2025) 70.8%, human avg 88.9% (benchmark report / lab release; flagged as reported/internal where applicable).</li>\\n    <li>MindCube spatial/working‑memory benchmark — reported GPT‑4o 38.8%, GPT‑5 59.7% (benchmark report).</li>\\n    <li>SimpleQA factuality/hallucination comparison — GPT‑5 reported >30% hallucination rate; other models (Anthropic Claude variants) report lower rates (vendor/benchmark reports).</li>\\n    <li>METR endurance test — reported GPT‑5.1‑Codex‑Max sustained autonomous performance ~2 hours 42 minutes vs GPT‑4 few minutes (internal lab test; provisional).</li>\\n    <li>DeepMind Gemini (’Deep Think’ mode) — reported solving 5 of 6 IMO 2025 problems within 4.5 hours (DeepMind report; task‑constrained result).</li>\\n    <li>Epoch AI compute projection — suggested ~2×10^29 FLOPs feasible by 2030 under some assumptions; other reports give upper bounds up to ~3×10^31 FLOPs (compute projection studies).</li>\\n    <li>RAND modeling of US–China race — reported Dec 1, 2025 (prisoner’s dilemma framing; policy analysis report).</li>\\n    <li>Expert surveys and timeline aggregates — multiple surveys report medians often in 2040–2060 with notable variance (survey meta‑analyses / aggregated studies).</li>\\n  </ul>\\n\\n  <p><em>Notes:</em> Where a result was described in the original draft as coming from “internal tests” or a single lab, I preserved the claim but flagged it above as provisional and recommended independent verification. For any use beyond classroom discussion, consult the original reports and benchmark datasets to confirm methodology, sample sizes, dates and reproducibility.</p>\\n</body>\\n</html>'}\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"query\": \"latest developments towards AGI\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b63632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"initial_report.html\", encoding = 'utf-8', mode = 'w') as f:\n",
    "    f.write(result['initial_draft'])\n",
    "\n",
    "with open(\"final_report.html\", encoding = 'utf-8', mode = 'w') as f:\n",
    "    f.write(result['final_draft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd605ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
